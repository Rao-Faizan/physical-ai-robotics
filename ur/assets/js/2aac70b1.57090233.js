"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[194],{2064:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-04-vla/week-13-capstone-implementation","title":"\u06c1\u0641\u062a\u06c1 13: \u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646 - \u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646","description":"\u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-04-vla/week-13-capstone-implementation.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-13-capstone-implementation","permalink":"/physical-ai-robotics/ur/docs/module-04-vla/week-13-capstone-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-04-vla/week-13-capstone-implementation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\u06c1\u0641\u062a\u06c1 13: \u06a9\u06cc\u067e\u0633\u0679\u0648\u0646 \u067e\u0631\u0648\u062c\u06cc\u06a9\u0679 - \u06af\u0641\u062a\u06af\u0648 \u06a9\u06d2 AI \u06a9\u06d2 \u0633\u0627\u062a\u06be \u062e\u0648\u062f\u06a9\u0627\u0631 \u06c1\u06cc\u0648\u0645\u0646\u0648\u0627\u0626\u0688","permalink":"/physical-ai-robotics/ur/docs/module-04-vla/week-13-capstone-intro"}}');var o=r(4848),a=r(8453);const s={},i="\u06c1\u0641\u062a\u06c1 13: \u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646 - \u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646",l={},c=[{value:"\u0642\u062f\u0645 \u0628\u06c1 \u0642\u062f\u0645 \u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646 \u06af\u0627\u0626\u06cc\u0688",id:"\u0642\u062f\u0645-\u0628\u06c1-\u0642\u062f\u0645-\u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646-\u06af\u0627\u0626\u06cc\u0688",level:2},{value:"\u062d\u0635\u06c1 1: Voice-to-Intent Pipeline",id:"\u062d\u0635\u06c1-1-voice-to-intent-pipeline",level:2},{value:"Wake-Word Detection \u0627\u0648\u0631 Command Capture",id:"wake-word-detection-\u0627\u0648\u0631-command-capture",level:3},{value:"\u062d\u0635\u06c1 2: LLM Planning Integration",id:"\u062d\u0635\u06c1-2-llm-planning-integration",level:2},{value:"ROS 2 \u06a9\u06d2 \u0633\u0627\u062a\u06be ReAct Task Executor",id:"ros-2-\u06a9\u06d2-\u0633\u0627\u062a\u06be-react-task-executor",level:3},{value:"\u062d\u0635\u06c1 3: Text-to-Speech Feedback Node",id:"\u062d\u0635\u06c1-3-text-to-speech-feedback-node",level:2},{value:"\u062d\u0635\u06c1 4: \u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u06a9\u06d2 \u0644\u06cc\u06d2 Launch File",id:"\u062d\u0635\u06c1-4-\u0645\u06a9\u0645\u0644-\u0633\u0633\u0679\u0645-\u06a9\u06d2-\u0644\u06cc\u06d2-launch-file",level:2},{value:"\u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u0686\u0644\u0627\u0646\u0627",id:"\u0645\u06a9\u0645\u0644-\u0633\u0633\u0679\u0645-\u0686\u0644\u0627\u0646\u0627",level:2},{value:"\u0633\u0633\u0679\u0645 \u06a9\u06cc \u062c\u0627\u0646\u0686",id:"\u0633\u0633\u0679\u0645-\u06a9\u06cc-\u062c\u0627\u0646\u0686",level:2},{value:"\u0679\u06cc\u0633\u0679 1: \u0633\u0627\u062f\u06c1 Fetch Task",id:"\u0679\u06cc\u0633\u0679-1-\u0633\u0627\u062f\u06c1-fetch-task",level:3},{value:"\u0679\u06cc\u0633\u0679 2: \u06a9\u062b\u06cc\u0631 \u0645\u0631\u062d\u0644\u06c1 Task",id:"\u0679\u06cc\u0633\u0679-2-\u06a9\u062b\u06cc\u0631-\u0645\u0631\u062d\u0644\u06c1-task",level:3},{value:"\u0679\u06cc\u0633\u0679 3: Adaptive Replanning",id:"\u0679\u06cc\u0633\u0679-3-adaptive-replanning",level:3},{value:"Optimization \u0627\u0648\u0631 Debugging",id:"optimization-\u0627\u0648\u0631-debugging",level:2},{value:"\u062e\u0644\u0627\u0635\u06c1",id:"\u062e\u0644\u0627\u0635\u06c1",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"\u06c1\u0641\u062a\u06c1-13-\u06a9\u06cc\u067e-\u0633\u0679\u0648\u0646-\u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646---\u0645\u06a9\u0645\u0644-\u0633\u0633\u0679\u0645-\u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646",children:"\u06c1\u0641\u062a\u06c1 13: \u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646 - \u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"\u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0627\u0645\u067e\u0644\u06cc\u0645\u0646\u0679\u06cc\u0634\u0646",src:r(2071).A+"",width:"736",height:"736"})}),"\n",(0,o.jsx)(n.h2,{id:"\u0642\u062f\u0645-\u0628\u06c1-\u0642\u062f\u0645-\u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646-\u06af\u0627\u0626\u06cc\u0688",children:"\u0642\u062f\u0645 \u0628\u06c1 \u0642\u062f\u0645 \u0627\u0646\u0679\u06cc\u06af\u0631\u06cc\u0634\u0646 \u06af\u0627\u0626\u06cc\u0688"}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 \u0628\u0627\u0628 VLA \u06a9\u06cc\u067e \u0633\u0679\u0648\u0646 \u0633\u0633\u0679\u0645 \u06a9\u06cc \u0645\u06a9\u0645\u0644 implementation \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u062c\u0648 voice recognition\u060c LLM planning\u060c visual grounding\u060c navigation\u060c manipulation\u060c \u0627\u0648\u0631 natural language feedback \u06a9\u0648 \u06cc\u06a9\u062c\u0627 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4 \u06c1\u0631 component \u067e\u0686\u06be\u0644\u06d2 modules \u067e\u0631 \u0628\u0646\u062a\u0627 \u06c1\u06d2 \u062a\u0627\u06a9\u06c1 \u0627\u06cc\u06a9 \u0645\u06a9\u0645\u0644 \u0637\u0648\u0631 \u067e\u0631 \u062e\u0648\u062f\u06a9\u0627\u0631 humanoid assistant \u062a\u06cc\u0627\u0631 \u06a9\u06cc\u0627 \u062c\u0627\u0626\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"\u062d\u0635\u06c1-1-voice-to-intent-pipeline",children:"\u062d\u0635\u06c1 1: Voice-to-Intent Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"wake-word-detection-\u0627\u0648\u0631-command-capture",children:"Wake-Word Detection \u0627\u0648\u0631 Command Capture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport sounddevice as sd\r\nimport numpy as np\r\nimport threading\r\nimport queue\r\n\r\nclass VoiceCommandNode(Node):\r\n    """\r\n    \u0645\u0633\u0644\u0633\u0644 voice command recognition \u06a9\u06d2 \u0644\u06cc\u06d2 ROS 2 node\u06d4\r\n    Wake-word detection \u0627\u0648\u0631 Whisper transcription \u06a9\u0648 implement \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n\r\n        # Transcribed commands \u06a9\u06d2 \u0644\u06cc\u06d2 Publisher\r\n        self.command_pub = self.create_publisher(String, \'/voice_commands\', 10)\r\n\r\n        # Speech recognition \u06a9\u06d2 \u0644\u06cc\u06d2 Whisper model load \u06a9\u0631\u06cc\u06ba\r\n        self.get_logger().info("Loading Whisper model...")\r\n        self.whisper_model = whisper.load_model("base")\r\n\r\n        # Audio configuration\r\n        self.sample_rate = 16000\r\n        self.wake_word = "hey robot"\r\n        self.listening = False\r\n\r\n        # Background recording \u06a9\u06d2 \u0644\u06cc\u06d2 Audio queue\r\n        self.audio_queue = queue.Queue()\r\n\r\n        # Audio capture thread \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba\r\n        self.audio_thread = threading.Thread(target=self._audio_capture_loop, daemon=True)\r\n        self.audio_thread.start()\r\n\r\n        self.get_logger().info("Voice command system ready. Say \'Hey Robot\' to activate.")\r\n\r\n    def _audio_capture_loop(self):\r\n        """\r\n        \u0645\u0633\u0644\u0633\u0644 audio capture \u06a9\u0631\u06cc\u06ba \u0627\u0648\u0631 wake-word detection \u06a9\u06d2 \u0644\u06cc\u06d2 process \u06a9\u0631\u06cc\u06ba\u06d4\r\n        ROS callbacks \u06a9\u0648 block \u06a9\u0631\u0646\u06d2 \u0633\u06d2 \u0628\u0686\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 background thread \u0645\u06cc\u06ba \u0686\u0644\u062a\u0627 \u06c1\u06d2\u06d4\r\n        """\r\n        while True:\r\n            # 3-\u0633\u06cc\u06a9\u0646\u0688 \u06a9\u06d2 chunks record \u06a9\u0631\u06cc\u06ba\r\n            audio = sd.rec(\r\n                int(3 * self.sample_rate),\r\n                samplerate=self.sample_rate,\r\n                channels=1,\r\n                dtype=\'float32\'\r\n            )\r\n            sd.wait()\r\n\r\n            # Audio \u06a9\u0648 transcribe \u06a9\u0631\u06cc\u06ba\r\n            audio_flat = audio.flatten()\r\n            result = self.whisper_model.transcribe(\r\n                audio_flat,\r\n                language=\'en\',\r\n                fp16=False  # CPU compatibility\r\n            )\r\n\r\n            transcription = result[\'text\'].lower().strip()\r\n            self.get_logger().debug(f"Heard: {transcription}")\r\n\r\n            # Wake-word \u06a9\u06d2 \u0644\u06cc\u06d2 check \u06a9\u0631\u06cc\u06ba\r\n            if self.wake_word in transcription:\r\n                self.get_logger().info("Wake-word detected! Listening for command...")\r\n                self._capture_command()\r\n\r\n    def _capture_command(self):\r\n        """\r\n        Wake-word \u06a9\u06d2 \u0628\u0639\u062f command \u06a9\u06d2 \u0644\u06cc\u06d2 \u0644\u0645\u0628\u0627 audio segment record \u06a9\u0631\u06cc\u06ba\u06d4\r\n        """\r\n        # 5-\u0633\u06cc\u06a9\u0646\u0688 \u06a9\u06cc command record \u06a9\u0631\u06cc\u06ba\r\n        self.get_logger().info("Recording command...")\r\n        audio = sd.rec(\r\n            int(5 * self.sample_rate),\r\n            samplerate=self.sample_rate,\r\n            channels=1,\r\n            dtype=\'float32\'\r\n        )\r\n        sd.wait()\r\n\r\n        # Command \u06a9\u0648 transcribe \u06a9\u0631\u06cc\u06ba\r\n        audio_flat = audio.flatten()\r\n        result = self.whisper_model.transcribe(audio_flat, language=\'en\', fp16=False)\r\n\r\n        command = result[\'text\'].strip()\r\n        self.get_logger().info(f"Command received: {command}")\r\n\r\n        # Planning system \u06a9\u0648 command publish \u06a9\u0631\u06cc\u06ba\r\n        msg = String()\r\n        msg.data = command\r\n        self.command_pub.publish(msg)\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = VoiceCommandNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"\u062d\u0635\u06c1-2-llm-planning-integration",children:"\u062d\u0635\u06c1 2: LLM Planning Integration"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-\u06a9\u06d2-\u0633\u0627\u062a\u06be-react-task-executor",children:"ROS 2 \u06a9\u06d2 \u0633\u0627\u062a\u06be ReAct Task Executor"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport os\r\nimport json\r\nfrom typing import Dict, List, Tuple\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\n\r\nopenai.api_key = os.getenv("OPENAI_API_KEY")\r\n\r\nclass VLAExecutorNode(Node):\r\n    """\r\n    LLM planning \u06a9\u0648 robot actions \u06a9\u06d2 \u0633\u0627\u062a\u06be integrate \u06a9\u0631\u0646\u06d2 \u0648\u0627\u0644\u0627 Main execution node\u06d4\r\n    Adaptive task execution \u06a9\u06d2 \u0644\u06cc\u06d2 ReAct pattern \u06a9\u0648 implement \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'vla_executor\')\r\n\r\n        # Voice commands \u06a9\u0648 subscribe \u06a9\u0631\u06cc\u06ba\r\n        self.cmd_sub = self.create_subscription(\r\n            String,\r\n            \'/voice_commands\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        # Visual grounding \u06a9\u06d2 \u0644\u06cc\u06d2 camera feed \u06a9\u0648 subscribe \u06a9\u0631\u06cc\u06ba\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Robot control \u06a9\u06d2 \u0644\u06cc\u06d2 Publishers\r\n        self.nav_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\r\n        self.grasp_pub = self.create_publisher(String, \'/grasp_command\', 10)\r\n        self.speech_pub = self.create_publisher(String, \'/tts_output\', 10)\r\n\r\n        # State tracking\r\n        self.current_image = None\r\n        self.bridge = CvBridge()\r\n        self.world_state = {\r\n            "robot_location": "living_room",\r\n            "held_object": None,\r\n            "known_objects": {}\r\n        }\r\n\r\n        self.execution_history = []\r\n        self.get_logger().info("VLA Executor ready.")\r\n\r\n    def image_callback(self, msg):\r\n        """Visual grounding \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0627\u0632\u06c1 \u062a\u0631\u06cc\u0646 camera image store \u06a9\u0631\u06cc\u06ba\u06d4"""\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\r\n\r\n    def command_callback(self, msg):\r\n        """\r\n        Voice command \u0648\u0635\u0648\u0644 \u06a9\u0631\u06cc\u06ba \u0627\u0648\u0631 ReAct loop \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 execute \u06a9\u0631\u06cc\u06ba\u06d4\r\n\r\n        Args:\r\n            msg: Natural language command \u067e\u0631 \u0645\u0634\u062a\u0645\u0644 String message\r\n        """\r\n        command = msg.data\r\n        self.get_logger().info(f"Executing command: {command}")\r\n\r\n        # GPT-4 \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0627\u0628\u062a\u062f\u0627\u0626\u06cc plan \u0628\u0646\u0627\u0626\u06cc\u06ba\r\n        plan = self._generate_plan(command)\r\n\r\n        # ReAct loop \u06a9\u06d2 \u0633\u0627\u062a\u06be plan execute \u06a9\u0631\u06cc\u06ba\r\n        success = self._execute_react_loop(command, plan)\r\n\r\n        if success:\r\n            self._speak("Task completed successfully!")\r\n        else:\r\n            self._speak("I encountered a problem and couldn\'t complete the task.")\r\n\r\n    def _generate_plan(self, goal: str) -> List[Dict]:\r\n        """\r\n        Goal \u06a9\u0648 action sequence \u0645\u06cc\u06ba decompose \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 GPT-4 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba\u06d4\r\n\r\n        Returns:\r\n            \'type\', \'parameters\' \u06a9\u06d2 \u0633\u0627\u062a\u06be action dictionaries \u06a9\u06cc List\r\n        """\r\n        system_prompt = """\u0622\u067e \u0627\u06cc\u06a9 humanoid robot \u06a9\u06d2 \u0644\u06cc\u06d2 task planner \u06c1\u06cc\u06ba\u06d4\r\n        \u0627\u0646 actions \u06a9\u0627 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 step-by-step plans \u0628\u0646\u0627\u0626\u06cc\u06ba:\r\n\r\n        - navigate(location): Location \u067e\u0631 \u062c\u0627\u0626\u06cc\u06ba\r\n        - locate(object_description): Camera \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 object \u062a\u0644\u0627\u0634 \u06a9\u0631\u06cc\u06ba\r\n        - approach(object): Manipulation \u06a9\u06d2 \u0644\u06cc\u06d2 object \u06a9\u06d2 \u0642\u0631\u06cc\u0628 \u062c\u0627\u0626\u06cc\u06ba\r\n        - grasp(object): Object \u0627\u0679\u06be\u0627\u0626\u06cc\u06ba\r\n        - place(object, location): Object \u0631\u06a9\u06be\u06cc\u06ba\r\n        - check(condition): Environment state \u06a9\u06cc \u062a\u0635\u062f\u06cc\u0642 \u06a9\u0631\u06cc\u06ba\r\n\r\n        JSON array \u0648\u0627\u067e\u0633 \u06a9\u0631\u06cc\u06ba: [{"action": "type", "params": {...}}, ...]\r\n        """\r\n\r\n        user_prompt = f"""Goal: {goal}\r\n\r\n        \u0645\u0648\u062c\u0648\u062f\u06c1 state:\r\n        - Robot \u06a9\u06cc location: {self.world_state[\'robot_location\']}\r\n        - \u06c1\u0627\u062a\u06be \u0645\u06cc\u06ba object: {self.world_state[\'held_object\']}\r\n\r\n        Action plan \u0628\u0646\u0627\u0626\u06cc\u06ba:\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {"role": "system", "content": system_prompt},\r\n                {"role": "user", "content": user_prompt}\r\n            ],\r\n            temperature=0.2\r\n        )\r\n\r\n        # JSON plan \u06a9\u0648 parse \u06a9\u0631\u06cc\u06ba\r\n        plan_text = response.choices[0].message.content\r\n        try:\r\n            # Response \u0633\u06d2 JSON \u0646\u06a9\u0627\u0644\u06cc\u06ba\r\n            start = plan_text.find(\'[\')\r\n            end = plan_text.rfind(\']\') + 1\r\n            plan = json.loads(plan_text[start:end])\r\n            return plan\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error("Failed to parse plan from LLM")\r\n            return []\r\n\r\n    def _execute_react_loop(self, goal: str, initial_plan: List[Dict]) -> bool:\r\n        """\r\n        Observations \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 adaptive replanning \u06a9\u06d2 \u0633\u0627\u062a\u06be plan execute \u06a9\u0631\u06cc\u06ba\u06d4\r\n\r\n        Args:\r\n            goal: \u0627\u0635\u0644 user goal\r\n            initial_plan: LLM \u0633\u06d2 \u0627\u0628\u062a\u062f\u0627\u0626\u06cc action sequence\r\n\r\n        Returns:\r\n            bool: \u0627\u06af\u0631 goal \u062d\u0627\u0635\u0644 \u06c1\u0648 \u062a\u0648 True\r\n        """\r\n        plan = initial_plan\r\n        step = 0\r\n        max_steps = 20\r\n\r\n        while step < max_steps:\r\n            if not plan:\r\n                self.get_logger().info("Plan complete!")\r\n                return True\r\n\r\n            # \u0627\u06af\u0644\u0627 action \u062d\u0627\u0635\u0644 \u06a9\u0631\u06cc\u06ba\r\n            action = plan[0]\r\n            self.get_logger().info(f"Step {step}: {action}")\r\n\r\n            # Action execute \u06a9\u0631\u06cc\u06ba \u0627\u0648\u0631 observation \u062d\u0627\u0635\u0644 \u06a9\u0631\u06cc\u06ba\r\n            success, observation = self._execute_action(action)\r\n\r\n            # Execution history \u06a9\u0648 update \u06a9\u0631\u06cc\u06ba\r\n            self.execution_history.append({\r\n                "step": step,\r\n                "action": action,\r\n                "success": success,\r\n                "observation": observation\r\n            })\r\n\r\n            if success:\r\n                # \u0645\u06a9\u0645\u0644 \u0634\u062f\u06c1 action \u06a9\u0648 plan \u0633\u06d2 \u06c1\u0679\u0627\u0626\u06cc\u06ba\r\n                plan = plan[1:]\r\n                self._speak(f"Completed: {action[\'action\']}")\r\n            else:\r\n                # \u0646\u0627\u06a9\u0627\u0645\u06cc \u067e\u0631 replan \u06a9\u0631\u06cc\u06ba\r\n                self.get_logger().warn(f"Action failed: {observation}")\r\n                self._speak(f"Hmm, {observation}. Let me try a different approach.")\r\n\r\n                # \u0645\u0648\u062c\u0648\u062f\u06c1 state \u0633\u06d2 \u0646\u06cc\u0627 plan \u0628\u0646\u0627\u0626\u06cc\u06ba\r\n                plan = self._replan(goal, observation)\r\n\r\n                if not plan:\r\n                    self.get_logger().error("Unable to replan. Task failed.")\r\n                    return False\r\n\r\n            step += 1\r\n\r\n        self.get_logger().warn("Max steps reached without completion.")\r\n        return False\r\n\r\n    def _execute_action(self, action: Dict) -> Tuple[bool, str]:\r\n        """\r\n        \u0627\u06cc\u06a9 action execute \u06a9\u0631\u06cc\u06ba \u0627\u0648\u0631 \u0646\u062a\u06cc\u062c\u06c1 \u0648\u0627\u067e\u0633 \u06a9\u0631\u06cc\u06ba\u06d4\r\n\r\n        Returns:\r\n            (success, observation): Execution \u06a9\u0627 \u0646\u062a\u06cc\u062c\u06c1 \u0627\u0648\u0631 observation\r\n        """\r\n        action_type = action[\'action\']\r\n        params = action.get(\'params\', {})\r\n\r\n        if action_type == \'navigate\':\r\n            return self._navigate(params[\'location\'])\r\n\r\n        elif action_type == \'locate\':\r\n            return self._locate_object(params[\'object_description\'])\r\n\r\n        elif action_type == \'approach\':\r\n            return self._approach_object(params[\'object\'])\r\n\r\n        elif action_type == \'grasp\':\r\n            return self._grasp_object(params[\'object\'])\r\n\r\n        elif action_type == \'place\':\r\n            return self._place_object(params[\'location\'])\r\n\r\n        elif action_type == \'check\':\r\n            return self._check_condition(params[\'condition\'])\r\n\r\n        else:\r\n            return False, f"\u0646\u0627\u0645\u0639\u0644\u0648\u0645 action type: {action_type}"\r\n\r\n    def _navigate(self, location: str) -> Tuple[bool, str]:\r\n        """Nav2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 location \u067e\u0631 \u062c\u0627\u0626\u06cc\u06ba\u06d4"""\r\n        self.get_logger().info(f"Navigating to {location}...")\r\n\r\n        # \u062d\u0642\u06cc\u0642\u06cc implementation \u0645\u06cc\u06ba: nav goal publish \u06a9\u0631\u06cc\u06ba \u0627\u0648\u0631 \u0646\u062a\u06cc\u062c\u06c1 \u06a9\u0627 \u0627\u0646\u062a\u0638\u0627\u0631 \u06a9\u0631\u06cc\u06ba\r\n        # \u0645\u0638\u0627\u06c1\u0631\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 Mock success\r\n        self.world_state[\'robot_location\'] = location\r\n        return True, f"{location} \u067e\u0631 \u067e\u06c1\u0646\u0686 \u06af\u0626\u06d2"\r\n\r\n    def _locate_object(self, description: str) -> Tuple[bool, str]:\r\n        """\r\n        Camera view \u0645\u06cc\u06ba object \u062a\u0644\u0627\u0634 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 CLIP \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba\u06d4\r\n\r\n        Returns:\r\n            (success, observation): \u0645\u0644\u0627 \u06cc\u0627 \u0646\u06c1\u06cc\u06ba \u0627\u0648\u0631 location \u06a9\u06cc \u062a\u0641\u0635\u06cc\u0644\r\n        """\r\n        if self.current_image is None:\r\n            return False, "\u06a9\u0648\u0626\u06cc camera image \u062f\u0633\u062a\u06cc\u0627\u0628 \u0646\u06c1\u06cc\u06ba"\r\n\r\n        # CLIP visual grounding \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba (\u06c1\u0641\u062a\u06c1 12 \u0633\u06d2)\r\n        # \u0645\u0638\u0627\u06c1\u0631\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0622\u0633\u0627\u0646 \u0628\u0646\u0627\u06cc\u0627 \u06af\u06cc\u0627\r\n        from PIL import Image as PILImage\r\n        import clip\r\n        import torch\r\n\r\n        device = "cuda" if torch.cuda.is_available() else "cpu"\r\n        model, preprocess = clip.load("ViT-B/32", device=device)\r\n\r\n        # Image \u06a9\u0648 convert \u06a9\u0631\u06cc\u06ba\r\n        pil_image = PILImage.fromarray(self.current_image)\r\n        image_input = preprocess(pil_image).unsqueeze(0).to(device)\r\n\r\n        # Object \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0644\u0627\u0634 \u06a9\u0631\u06cc\u06ba\r\n        text_input = clip.tokenize([description, "empty scene"]).to(device)\r\n\r\n        with torch.no_grad():\r\n            image_features = model.encode_image(image_input)\r\n            text_features = model.encode_text(text_input)\r\n\r\n            image_features /= image_features.norm(dim=-1, keepdim=True)\r\n            text_features /= text_features.norm(dim=-1, keepdim=True)\r\n\r\n            similarity = (image_features @ text_features.T).squeeze(0)\r\n            probs = similarity.softmax(dim=0)\r\n\r\n        # Confidence \u06a9\u06d2 \u0633\u0627\u062a\u06be object \u06a9\u0627 \u067e\u062a\u06c1 \u0644\u06af\u0627\u06cc\u0627 \u06af\u06cc\u0627 \u06c1\u06d2 \u06cc\u0627 \u0646\u06c1\u06cc\u06ba check \u06a9\u0631\u06cc\u06ba\r\n        if probs[0] > 0.7:\r\n            # World state \u0645\u06cc\u06ba store \u06a9\u0631\u06cc\u06ba\r\n            self.world_state[\'known_objects\'][description] = {\r\n                "location": self.world_state[\'robot_location\'],\r\n                "confidence": float(probs[0])\r\n            }\r\n            return True, f"{description} \u0645\u0644\u0627 confidence {probs[0]:.2f} \u06a9\u06d2 \u0633\u0627\u062a\u06be"\r\n        else:\r\n            return False, f"View \u0645\u06cc\u06ba {description} \u0646\u06c1\u06cc\u06ba \u0645\u0644 \u0633\u06a9\u0627"\r\n\r\n    def _grasp_object(self, object_name: str) -> Tuple[bool, str]:\r\n        """MoveIt2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 grasp execute \u06a9\u0631\u06cc\u06ba\u06d4"""\r\n        self.get_logger().info(f"Grasping {object_name}...")\r\n\r\n        # Manipulation controller \u06a9\u0648 grasp command publish \u06a9\u0631\u06cc\u06ba\r\n        msg = String()\r\n        msg.data = f"grasp:{object_name}"\r\n        self.grasp_pub.publish(msg)\r\n\r\n        # Mock success\r\n        self.world_state[\'held_object\'] = object_name\r\n        return True, f"{object_name} \u06a9\u0627\u0645\u06cc\u0627\u0628\u06cc \u0633\u06d2 \u067e\u06a9\u0691\u0627 \u06af\u06cc\u0627"\r\n\r\n    def _place_object(self, location: str) -> Tuple[bool, str]:\r\n        """\u06c1\u0627\u062a\u06be \u0645\u06cc\u06ba \u0645\u0648\u062c\u0648\u062f object \u06a9\u0648 location \u067e\u0631 \u0631\u06a9\u06be\u06cc\u06ba\u06d4"""\r\n        if self.world_state[\'held_object\'] is None:\r\n            return False, "\u06a9\u0648\u0626\u06cc object \u06c1\u0627\u062a\u06be \u0645\u06cc\u06ba \u0646\u06c1\u06cc\u06ba"\r\n\r\n        object_name = self.world_state[\'held_object\']\r\n        self.get_logger().info(f"Placing {object_name} at {location}...")\r\n\r\n        # Place command publish \u06a9\u0631\u06cc\u06ba\r\n        msg = String()\r\n        msg.data = f"place:{location}"\r\n        self.grasp_pub.publish(msg)\r\n\r\n        self.world_state[\'held_object\'] = None\r\n        return True, f"{object_name} \u06a9\u0648 {location} \u067e\u0631 \u0631\u06a9\u06be \u062f\u06cc\u0627 \u06af\u06cc\u0627"\r\n\r\n    def _approach_object(self, object_name: str) -> Tuple[bool, str]:\r\n        """Manipulation \u06a9\u06d2 \u0644\u06cc\u06d2 object \u06a9\u06d2 \u0642\u0631\u06cc\u0628 \u062c\u0627\u0626\u06cc\u06ba\u06d4"""\r\n        if object_name not in self.world_state[\'known_objects\']:\r\n            return False, f"{object_name} \u06a9\u06cc location \u0646\u0627\u0645\u0639\u0644\u0648\u0645"\r\n\r\n        # Object \u06a9\u06cc location \u067e\u0631 navigate \u06a9\u0631\u06cc\u06ba\r\n        obj_info = self.world_state[\'known_objects\'][object_name]\r\n        return True, f"{object_name} \u06a9\u06d2 \u0642\u0631\u06cc\u0628 \u067e\u06c1\u0646\u0686 \u06af\u0626\u06d2"\r\n\r\n    def _check_condition(self, condition: str) -> Tuple[bool, str]:\r\n        """Vision/sensors \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 environmental condition \u06a9\u06cc \u062a\u0635\u062f\u06cc\u0642 \u06a9\u0631\u06cc\u06ba\u06d4"""\r\n        # Visual question answering \u06a9\u06d2 \u0644\u06cc\u06d2 GPT-4V \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba\r\n        # \u0645\u0638\u0627\u06c1\u0631\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0622\u0633\u0627\u0646 \u0628\u0646\u0627\u06cc\u0627 \u06af\u06cc\u0627\r\n        return True, f"\u0686\u06cc\u06a9 \u06a9\u06cc\u0627 \u06af\u06cc\u0627: {condition}"\r\n\r\n    def _replan(self, goal: str, failure_reason: str) -> List[Dict]:\r\n        """\r\n        \u0646\u0627\u06a9\u0627\u0645\u06cc \u06a9\u06cc observation \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 \u0646\u0627\u06a9\u0627\u0645\u06cc \u06a9\u06d2 \u0628\u0639\u062f \u0646\u06cc\u0627 plan \u0628\u0646\u0627\u0626\u06cc\u06ba\u06d4\r\n        """\r\n        prompt = f"""\u0627\u0635\u0644 goal: {goal}\r\n\r\n        \u067e\u0686\u06be\u0644\u0627 plan \u0646\u0627\u06a9\u0627\u0645 \u06c1\u0648 \u06af\u06cc\u0627 \u06a9\u06cc\u0648\u0646\u06a9\u06c1: {failure_reason}\r\n\r\n        Execution history:\r\n        {json.dumps(self.execution_history[-3:], indent=2)}\r\n\r\n        \u0645\u0648\u062c\u0648\u062f\u06c1 state:\r\n        {json.dumps(self.world_state, indent=2)}\r\n\r\n        \u0645\u062a\u0628\u0627\u062f\u0644 plan \u0628\u0637\u0648\u0631 JSON array \u0628\u0646\u0627\u0626\u06cc\u06ba:\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.3\r\n        )\r\n\r\n        # \u0646\u06cc\u0627 plan parse \u06a9\u0631\u06cc\u06ba\r\n        plan_text = response.choices[0].message.content\r\n        try:\r\n            start = plan_text.find(\'[\')\r\n            end = plan_text.rfind(\']\') + 1\r\n            new_plan = json.loads(plan_text[start:end])\r\n            return new_plan\r\n        except:\r\n            return []\r\n\r\n    def _speak(self, text: str):\r\n        """\r\n        TTS \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 speech output \u0628\u0646\u0627\u0626\u06cc\u06ba\u06d4\r\n        """\r\n        self.get_logger().info(f"Speaking: {text}")\r\n\r\n        # TTS node \u06a9\u0648 publish \u06a9\u0631\u06cc\u06ba\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = VLAExecutorNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"\u062d\u0635\u06c1-3-text-to-speech-feedback-node",children:"\u062d\u0635\u06c1 3: Text-to-Speech Feedback Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyttsx3\r\n\r\nclass TTSNode(Node):\r\n    """\r\n    Natural language feedback \u06a9\u06d2 \u0644\u06cc\u06d2 Text-to-Speech node\u06d4\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'tts_node\')\r\n\r\n        # Speech requests \u06a9\u0648 subscribe \u06a9\u0631\u06cc\u06ba\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/tts_output\',\r\n            self.speak_callback,\r\n            10\r\n        )\r\n\r\n        # TTS engine \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba\r\n        self.engine = pyttsx3.init()\r\n        self.engine.setProperty(\'rate\', 150)  # \u0627\u0644\u0641\u0627\u0638 \u0641\u06cc \u0645\u0646\u0679\r\n        self.engine.setProperty(\'volume\', 0.9)\r\n\r\n        self.get_logger().info("TTS system ready.")\r\n\r\n    def speak_callback(self, msg):\r\n        """Text \u06a9\u0648 speech \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u06cc\u06ba\u06d4"""\r\n        text = msg.data\r\n        self.get_logger().info(f"Speaking: {text}")\r\n\r\n        # Speech \u06a9\u0648 synthesize \u06a9\u0631\u06cc\u06ba (non-blocking)\r\n        self.engine.say(text)\r\n        self.engine.runAndWait()\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = TTSNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"\u062d\u0635\u06c1-4-\u0645\u06a9\u0645\u0644-\u0633\u0633\u0679\u0645-\u06a9\u06d2-\u0644\u06cc\u06d2-launch-file",children:"\u062d\u0635\u06c1 4: \u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u06a9\u06d2 \u0644\u06cc\u06d2 Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# launch/vla_capstone.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Voice command node\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='voice_command_node',\r\n            name='voice_command',\r\n            output='screen'\r\n        ),\r\n\r\n        # VLA executor (planning + vision + control)\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='vla_executor_node',\r\n            name='vla_executor',\r\n            output='screen',\r\n            parameters=[{\r\n                'use_sim_time': False\r\n            }]\r\n        ),\r\n\r\n        # Text-to-speech feedback\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='tts_node',\r\n            name='tts',\r\n            output='screen'\r\n        ),\r\n\r\n        # Camera driver (\u0627\u0635\u0644 camera node \u0633\u06d2 \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u06cc\u06ba)\r\n        Node(\r\n            package='usb_cam',\r\n            executable='usb_cam_node_exe',\r\n            name='camera',\r\n            parameters=[{\r\n                'video_device': '/dev/video0',\r\n                'framerate': 30.0,\r\n                'image_width': 640,\r\n                'image_height': 480\r\n            }]\r\n        )\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"\u0645\u06a9\u0645\u0644-\u0633\u0633\u0679\u0645-\u0686\u0644\u0627\u0646\u0627",children:"\u0645\u06a9\u0645\u0644 \u0633\u0633\u0679\u0645 \u0686\u0644\u0627\u0646\u0627"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# \u0679\u0631\u0645\u06cc\u0646\u0644 1: \u0628\u0646\u06cc\u0627\u062f\u06cc robot systems \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba (simulation \u06cc\u0627 hardware)\r\nros2 launch your_robot_description robot.launch.py\r\n\r\n# \u0679\u0631\u0645\u06cc\u0646\u0644 2: VLA capstone system \u0634\u0631\u0648\u0639 \u06a9\u0631\u06cc\u06ba\r\nros2 launch vla_capstone vla_capstone.launch.py\r\n\r\n# \u0679\u0631\u0645\u06cc\u0646\u0644 3: Execution \u06a9\u0648 monitor \u06a9\u0631\u06cc\u06ba\r\nros2 topic echo /voice_commands\r\nros2 topic echo /tts_output\r\n\r\n# \u0679\u0631\u0645\u06cc\u0646\u0644 4: Visualization\r\nrviz2 -d vla_capstone.rviz\n"})}),"\n",(0,o.jsx)(n.h2,{id:"\u0633\u0633\u0679\u0645-\u06a9\u06cc-\u062c\u0627\u0646\u0686",children:"\u0633\u0633\u0679\u0645 \u06a9\u06cc \u062c\u0627\u0646\u0686"}),"\n",(0,o.jsx)(n.h3,{id:"\u0679\u06cc\u0633\u0679-1-\u0633\u0627\u062f\u06c1-fetch-task",children:"\u0679\u06cc\u0633\u0679 1: \u0633\u0627\u062f\u06c1 Fetch Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u06a9\u0645\u0627\u0646\u0688"}),': "Hey robot, bring me the red mug from the table."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u0645\u062a\u0648\u0642\u0639 \u0631\u0648\u06cc\u06c1"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'\u0633\u0633\u0679\u0645 \u062a\u0635\u062f\u06cc\u0642 \u06a9\u0631\u062a\u0627 \u06c1\u06d2: "Navigating to table..."'}),"\n",(0,o.jsx)(n.li,{children:"CLIP \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631 \u06a9\u06d2 \u0633\u0631\u062e mug \u06a9\u0627 \u067e\u062a\u06c1 \u0644\u06af\u0627\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(n.li,{children:"\u0642\u0631\u06cc\u0628 \u062c\u0627\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 mug \u067e\u06a9\u0691\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(n.li,{children:'\u0648\u0627\u067e\u0633 \u0622\u062a\u0627 \u06c1\u06d2: "Here is your red mug."'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"\u0679\u06cc\u0633\u0679-2-\u06a9\u062b\u06cc\u0631-\u0645\u0631\u062d\u0644\u06c1-task",children:"\u0679\u06cc\u0633\u0679 2: \u06a9\u062b\u06cc\u0631 \u0645\u0631\u062d\u0644\u06c1 Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u06a9\u0645\u0627\u0646\u0688"}),': "Clear the desk by moving all mugs to the dish rack."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u0645\u062a\u0648\u0642\u0639 \u0631\u0648\u06cc\u06c1"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Desk \u067e\u0631 \u062c\u0627\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(n.li,{children:"\u0628\u0627\u0631 \u0628\u0627\u0631 mugs \u06a9\u0627 \u067e\u062a\u06c1 \u0644\u06af\u0627\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(n.li,{children:"\u06c1\u0631 mug \u06a9\u06d2 \u0644\u06cc\u06d2: \u067e\u06a9\u0691\u0646\u0627 \u2192 dish rack \u067e\u0631 \u062c\u0627\u0646\u0627 \u2192 \u0631\u06a9\u06be\u0646\u0627"}),"\n",(0,o.jsx)(n.li,{children:'\u062a\u0635\u062f\u06cc\u0642 \u06a9\u0631\u062a\u0627 \u06c1\u06d2: "Desk cleared. Moved 3 mugs to dish rack."'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"\u0679\u06cc\u0633\u0679-3-adaptive-replanning",children:"\u0679\u06cc\u0633\u0679 3: Adaptive Replanning"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u06a9\u0645\u0627\u0646\u0688"}),': "Get the laptop from the bedroom."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u0645\u062a\u0648\u0642\u0639 \u0631\u0648\u06cc\u06c1"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Bedroom \u0645\u06cc\u06ba \u062c\u0627\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(n.li,{children:'\u0627\u06af\u0631 laptop \u0646\u06c1\u06cc\u06ba \u0645\u0644\u062a\u0627: "I don\'t see a laptop in the bedroom. Should I check another room?"'}),"\n",(0,o.jsx)(n.li,{children:'\u0635\u0627\u0631\u0641: "Try the living room."'}),"\n",(0,o.jsx)(n.li,{children:"Replan \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 living room \u0645\u06cc\u06ba \u062a\u0644\u0627\u0634 \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"optimization-\u0627\u0648\u0631-debugging",children:"Optimization \u0627\u0648\u0631 Debugging"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Tuning"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\u062a\u06cc\u0632 \u062a\u0631 planning \u06a9\u06d2 \u0644\u06cc\u06d2 ",(0,o.jsx)(n.code,{children:"gpt-3.5-turbo"})," \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba (GPT-4 \u0633\u06d2 3x speedup)"]}),"\n",(0,o.jsx)(n.li,{children:"\u0645\u0639\u0644\u0648\u0645 objects \u06a9\u06d2 \u0644\u06cc\u06d2 CLIP embeddings cache \u06a9\u0631\u06cc\u06ba"}),"\n",(0,o.jsx)(n.li,{children:"Blocking \u0633\u06d2 \u0628\u0686\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0644\u06af thread \u0645\u06cc\u06ba perception \u0686\u0644\u0627\u0626\u06cc\u06ba"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u0639\u0627\u0645 \u0645\u0633\u0627\u0626\u0644"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper latency"}),": ",(0,o.jsx)(n.code,{children:"tiny"})," \u06cc\u0627 ",(0,o.jsx)(n.code,{children:"base"})," model \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba\u061b distil-whisper \u067e\u0631 \u063a\u0648\u0631 \u06a9\u0631\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"CLIP false positives"}),": Confidence threshold 0.8+ \u062a\u06a9 \u0628\u0691\u06be\u0627\u0626\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan failures"}),": System prompt \u0645\u06cc\u06ba \u0645\u0632\u06cc\u062f few-shot examples \u0634\u0627\u0645\u0644 \u06a9\u0631\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TTS blocking"}),": \u0627\u0644\u06af process \u0645\u06cc\u06ba \u0686\u0644\u0627\u0626\u06cc\u06ba \u06cc\u0627 async synthesis \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"\u062e\u0644\u0627\u0635\u06c1",children:"\u062e\u0644\u0627\u0635\u06c1"}),"\n",(0,o.jsx)(n.p,{children:"\u0627\u0628 \u0622\u067e \u0646\u06d2 speech recognition\u060c LLM planning\u060c visual grounding\u060c \u0627\u0648\u0631 robot control \u06a9\u0648 \u06cc\u06a9\u062c\u0627 \u06a9\u0631\u0646\u06d2 \u0648\u0627\u0644\u0627 \u0627\u06cc\u06a9 \u0645\u06a9\u0645\u0644 Vision-Language-Action \u0633\u0633\u0679\u0645 \u0628\u0646\u0627 \u0644\u06cc\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 embodied AI \u0645\u06cc\u06ba \u0645\u0648\u062c\u0648\u062f\u06c1 \u062c\u062f\u06cc\u062f \u062a\u0631\u06cc\u0646 \u0635\u0648\u0631\u062a\u062d\u0627\u0644 \u06a9\u06cc \u0646\u0645\u0627\u0626\u0646\u062f\u06af\u06cc \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u062c\u0648 \u0638\u0627\u06c1\u0631 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u06a9\u0633 \u0637\u0631\u062d foundation models (GPT-4\u060c CLIP\u060c Whisper) \u06a9\u0648 \u062d\u0642\u06cc\u0642\u06cc \u062f\u0646\u06cc\u0627 \u06a9\u06cc robotic applications \u06a9\u06d2 \u0644\u06cc\u06d2 \u062a\u0631\u062a\u06cc\u0628 \u062f\u06cc\u0627 \u062c\u0627 \u0633\u06a9\u062a\u0627 \u06c1\u06d2\u06d4"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"\u0627\u06af\u0644\u06d2 \u0627\u0642\u062f\u0627\u0645\u0627\u062a"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u062d\u0642\u06cc\u0642\u06cc humanoid platform \u067e\u0631 deploy \u06a9\u0631\u06cc\u06ba"}),"\n",(0,o.jsx)(n.li,{children:"\u0645\u0632\u06cc\u062f \u067e\u06cc\u0686\u06cc\u062f\u06c1 error recovery \u0634\u0627\u0645\u0644 \u06a9\u0631\u06cc\u06ba (human-in-the-loop requests)"}),"\n",(0,o.jsx)(n.li,{children:"Memory persistence \u06a9\u0648 implement \u06a9\u0631\u06cc\u06ba (sessions \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646 object locations \u06cc\u0627\u062f \u0631\u06a9\u06be\u06cc\u06ba)"}),"\n",(0,o.jsx)(n.li,{children:"\u062c\u062f\u06cc\u062f manipulation \u06a9\u0648 \u06cc\u06a9\u062c\u0627 \u06a9\u0631\u06cc\u06ba (dexterous grasping\u060c tool use)"}),"\n",(0,o.jsx)(n.li,{children:"\u06a9\u062b\u06cc\u0631 robot \u062a\u0639\u0627\u0648\u0646 \u062a\u06a9 \u062a\u0648\u0633\u06cc\u0639 \u06a9\u0631\u06cc\u06ba"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Module 4 \u0645\u06a9\u0645\u0644 \u06a9\u0631\u0646\u06d2 \u067e\u0631 \u0645\u0628\u0627\u0631\u06a9\u0628\u0627\u062f! \u0627\u0628 \u0622\u067e conversational AI \u0635\u0644\u0627\u062d\u06cc\u062a\u0648\u06ba \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0627\u06af\u0644\u06cc \u0646\u0633\u0644 \u06a9\u06d2 \u062e\u0648\u062f\u06a9\u0627\u0631 humanoid robots \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u0644\u06cc\u0633 \u06c1\u06cc\u06ba\u06d4"})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},2071:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/ai-16-9e7d952177784162a04ea56d55dcfdb5.png"},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>i});var t=r(6540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);