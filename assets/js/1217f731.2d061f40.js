"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[6031],{7319:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/ai-8-243c0894c91b74243651c9ea86ab6cf5.png"},7401:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-02-simulation/week-7-sensors","title":"Week 7: Sensor Simulation","description":"Sensor Simulation","source":"@site/docs/module-02-simulation/week-7-sensors.md","sourceDirName":"module-02-simulation","slug":"/module-02-simulation/week-7-sensors","permalink":"/physical-ai-robotics/docs/module-02-simulation/week-7-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-02-simulation/week-7-sensors.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 6: Physics Configuration for Humanoid Robots","permalink":"/physical-ai-robotics/docs/module-02-simulation/week-6-physics"},"next":{"title":"Week 7: Unity for Robotics","permalink":"/physical-ai-robotics/docs/module-02-simulation/week-7-unity"}}');var i=r(4848),a=r(8453);const t={},o="Week 7: Sensor Simulation",l={},c=[{value:"Why Sensor Simulation Matters",id:"why-sensor-simulation-matters",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Depth Cameras (Intel RealSense D435)",id:"depth-cameras-intel-realsense-d435",level:2},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:2},{value:"Joint Encoders and Noise Modeling",id:"joint-encoders-and-noise-modeling",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-7-sensor-simulation",children:"Week 7: Sensor Simulation"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Sensor Simulation",src:r(7319).A+"",width:"736",height:"1288"})}),"\n",(0,i.jsx)(n.h2,{id:"why-sensor-simulation-matters",children:"Why Sensor Simulation Matters"}),"\n",(0,i.jsx)(n.p,{children:"Physical AI systems rely on noisy, imperfect sensor data. Simulating sensors with realistic characteristics is critical for sim-to-real transfer. Key challenges:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise and Drift"}),": Real sensors have Gaussian noise, bias drift, and quantization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update Rates"}),": Sensors run at different frequencies (IMU: 200Hz, LiDAR: 10-20Hz, camera: 30Hz)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency"}),": Data arrives with delays (10-50ms typical)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environmental Effects"}),": Lighting changes affect cameras, reflective surfaces confuse LiDAR"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"})," measures distances by timing laser pulses. Common in outdoor navigation."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add LiDAR sensor to humanoid head --\x3e\r\n<link name="head">\r\n  <pose>0 0 1.7 0 0 0</pose> \x3c!-- 1.7m height --\x3e\r\n\r\n  <sensor name="lidar_sensor" type="gpu_lidar">\r\n    <pose>0 0 0.1 0 0 0</pose> \x3c!-- Offset from head center --\x3e\r\n    <update_rate>10</update_rate> \x3c!-- 10 Hz (common for robotics) --\x3e\r\n    <topic>/humanoid/lidar</topic>\r\n\r\n    <lidar>\r\n      <scan>\r\n        <horizontal>\r\n          <samples>720</samples> \x3c!-- 720 points per scan --\x3e\r\n          <resolution>1.0</resolution>\r\n          <min_angle>-3.14159</min_angle> \x3c!-- -180 degrees --\x3e\r\n          <max_angle>3.14159</max_angle>  \x3c!-- +180 degrees --\x3e\r\n        </horizontal>\r\n        <vertical>\r\n          <samples>16</samples> \x3c!-- 16 vertical layers --\x3e\r\n          <resolution>1.0</resolution>\r\n          <min_angle>-0.2618</min_angle> \x3c!-- -15 degrees --\x3e\r\n          <max_angle>0.2618</max_angle>  \x3c!-- +15 degrees --\x3e\r\n        </vertical>\r\n      </scan>\r\n\r\n      <range>\r\n        <min>0.1</min> \x3c!-- Minimum range: 10cm --\x3e\r\n        <max>30.0</max> \x3c!-- Maximum range: 30m --\x3e\r\n        <resolution>0.01</resolution> \x3c!-- 1cm precision --\x3e\r\n      </range>\r\n\r\n      \x3c!-- Realistic noise model --\x3e\r\n      <noise>\r\n        <type>gaussian</type>\r\n        <mean>0.0</mean>\r\n        <stddev>0.02</stddev> \x3c!-- 2cm standard deviation --\x3e\r\n      </noise>\r\n    </lidar>\r\n\r\n    <visualize>true</visualize> \x3c!-- Show rays in GUI --\x3e\r\n  </sensor>\r\n</link>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS2 Subscriber (Python)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\n\r\nclass LidarProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_processor')\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/humanoid/lidar',\r\n            self.lidar_callback,\r\n            10)\r\n\r\n    def lidar_callback(self, msg):\r\n        # Extract range data\r\n        ranges = msg.ranges  # List of distances (meters)\r\n        angles = [msg.angle_min + i * msg.angle_increment\r\n                  for i in range(len(ranges))]\r\n\r\n        # Detect closest obstacle\r\n        valid_ranges = [r for r in ranges if msg.range_min < r < msg.range_max]\r\n        if valid_ranges:\r\n            min_distance = min(valid_ranges)\r\n            self.get_logger().info(f'Closest obstacle: {min_distance:.2f}m')\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = LidarProcessor()\r\n    rclpy.spin(node)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"depth-cameras-intel-realsense-d435",children:"Depth Cameras (Intel RealSense D435)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Depth cameras"})," provide RGB images with per-pixel depth. Essential for manipulation and obstacle avoidance."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- RealSense D435 mounted on robot chest --\x3e\r\n<sensor name="depth_camera" type="depth_camera">\r\n  <pose>0.15 0 1.2 0 0.3 0</pose> \x3c!-- 0.3 rad (17\xb0) downward tilt --\x3e\r\n  <update_rate>30</update_rate> \x3c!-- 30 FPS --\x3e\r\n  <topic>/humanoid/depth</topic>\r\n\r\n  <camera>\r\n    <horizontal_fov>1.5184</horizontal_fov> \x3c!-- 87\xb0 (RealSense spec) --\x3e\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format> \x3c!-- RGB8 --\x3e\r\n    </image>\r\n    <clip>\r\n      <near>0.3</near> \x3c!-- Minimum depth: 30cm --\x3e\r\n      <far>10.0</far> \x3c!-- Maximum depth: 10m --\x3e\r\n    </clip>\r\n\r\n    \x3c!-- Depth-specific noise (increases with distance) --\x3e\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.007</stddev> \x3c!-- 7mm at 1m (realistic for D435) --\x3e\r\n    </noise>\r\n  </camera>\r\n\r\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_camera.so">\r\n    <ros>\r\n      <namespace>/humanoid</namespace>\r\n      <argument>image_raw:=rgb/image_raw</argument>\r\n      <argument>depth/image_raw:=depth/image_raw</argument>\r\n      <argument>camera_info:=rgb/camera_info</argument>\r\n    </ros>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing Depth Images (Python + OpenCV)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\r\nimport numpy as np\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\n\r\nclass DepthProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('depth_processor')\r\n        self.bridge = CvBridge()\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            '/humanoid/depth/image_raw',\r\n            self.depth_callback,\r\n            10)\r\n\r\n    def depth_callback(self, msg):\r\n        # Convert ROS Image to OpenCV format (32FC1 - float depth)\r\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\r\n\r\n        # Find graspable objects (20cm - 50cm range)\r\n        mask = cv2.inRange(depth_image, 0.2, 0.5)\r\n\r\n        # Morphological operations to clean up noise\r\n        kernel = np.ones((5,5), np.uint8)\r\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\r\n\r\n        # Find largest contour (potential grasp target)\r\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\r\n                                       cv2.CHAIN_APPROX_SIMPLE)\r\n        if contours:\r\n            largest = max(contours, key=cv2.contourArea)\r\n            M = cv2.moments(largest)\r\n            if M['m00'] > 0:\r\n                cx = int(M['m10'] / M['m00'])\r\n                cy = int(M['m01'] / M['m00'])\r\n                depth = depth_image[cy, cx]\r\n                self.get_logger().info(f'Target at ({cx}, {cy}), depth: {depth:.2f}m')\n"})}),"\n",(0,i.jsx)(n.h2,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,i.jsxs)(n.p,{children:["IMUs measure ",(0,i.jsx)(n.strong,{children:"linear acceleration"})," and ",(0,i.jsx)(n.strong,{children:"angular velocity"}),". Critical for balance control in humanoids."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU in robot torso (common placement) --\x3e\r\n<sensor name="imu_sensor" type="imu">\r\n  <pose>0 0 1.0 0 0 0</pose> \x3c!-- Center of mass --\x3e\r\n  <update_rate>200</update_rate> \x3c!-- 200 Hz (high-frequency control) --\x3e\r\n  <topic>/humanoid/imu</topic>\r\n\r\n  <imu>\r\n    \x3c!-- Accelerometer noise --\x3e\r\n    <linear_acceleration>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.017</stddev> \x3c!-- 0.017 m/s\xb2 (typical MEMS IMU) --\x3e\r\n          <bias_mean>0.05</bias_mean> \x3c!-- 50 mg bias drift --\x3e\r\n          <bias_stddev>0.01</bias_stddev>\r\n        </noise>\r\n      </x>\r\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.017</stddev></noise></y>\r\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.017</stddev></noise></z>\r\n    </linear_acceleration>\r\n\r\n    \x3c!-- Gyroscope noise --\x3e\r\n    <angular_velocity>\r\n      <x>\r\n        <noise type="gaussian">\r\n          <mean>0.0</mean>\r\n          <stddev>0.00087</stddev> \x3c!-- 0.05\xb0/s noise --\x3e\r\n          <bias_mean>0.0001</bias_mean> \x3c!-- Gyro drift --\x3e\r\n          <bias_stddev>0.00005</bias_stddev>\r\n        </noise>\r\n      </x>\r\n      <y><noise type="gaussian"><mean>0</mean><stddev>0.00087</stddev></noise></y>\r\n      <z><noise type="gaussian"><mean>0</mean><stddev>0.00087</stddev></noise></z>\r\n    </angular_velocity>\r\n  </imu>\r\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Orientation Estimation with Madgwick Filter"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from sensor_msgs.msg import Imu\r\nimport numpy as np\r\n\r\nclass OrientationEstimator:\r\n    def __init__(self, beta=0.1):\r\n        self.quaternion = np.array([1.0, 0.0, 0.0, 0.0])  # w, x, y, z\r\n        self.beta = beta  # Filter gain\r\n\r\n    def update(self, accel, gyro, dt):\r\n        # Madgwick AHRS algorithm (simplified)\r\n        # Normalize accelerometer\r\n        accel = accel / np.linalg.norm(accel)\r\n\r\n        # Gradient descent algorithm corrective step\r\n        q = self.quaternion\r\n        f = np.array([\r\n            2*(q[1]*q[3] - q[0]*q[2]) - accel[0],\r\n            2*(q[0]*q[1] + q[2]*q[3]) - accel[1],\r\n            2*(0.5 - q[1]**2 - q[2]**2) - accel[2]\r\n        ])\r\n\r\n        # Update quaternion with gyroscope\r\n        q_dot = 0.5 * self.quaternion_multiply(q, [0, gyro[0], gyro[1], gyro[2]])\r\n        q = q + q_dot * dt\r\n\r\n        # Normalize and store\r\n        self.quaternion = q / np.linalg.norm(q)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"joint-encoders-and-noise-modeling",children:"Joint Encoders and Noise Modeling"}),"\n",(0,i.jsxs)(n.p,{children:["Real joint encoders have ",(0,i.jsx)(n.strong,{children:"quantization"})," (discrete steps) and ",(0,i.jsx)(n.strong,{children:"backlash"})," (dead zone)."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Joint position sensor with realistic noise --\x3e\r\n<joint name="knee_joint" type="revolute">\r\n  <sensor name="knee_encoder" type="joint_position">\r\n    <update_rate>100</update_rate> \x3c!-- 100 Hz --\x3e\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.001</stddev> \x3c!-- 0.001 rad (~0.06\xb0) quantization --\x3e\r\n    </noise>\r\n  </sensor>\r\n</joint>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise"}),": Add all four sensors (LiDAR, depth camera, IMU, joint encoders) to a humanoid model. Create a ROS2 node that fuses IMU and joint encoder data to estimate the robot's tilt angle. Test with a push force applied to the torso."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-robotics/docs/module-02-simulation/week-7-unity",children:"Week 7 - Unity for Robotics"})," - High-fidelity simulation with Unity Robotics Hub."]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var s=r(6540);const i={},a=s.createContext(i);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);