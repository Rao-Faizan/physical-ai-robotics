"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[5327],{3596:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/ai-15-fa0e99e22ab2f0a1acfb70218da4aeeb.png"},4993:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-04-vla/week-13-capstone-intro","title":"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI","description":"Capstone Project Introduction","source":"@site/docs/module-04-vla/week-13-capstone-intro.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-13-capstone-intro","permalink":"/physical-ai-robotics/docs/module-04-vla/week-13-capstone-intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-04-vla/week-13-capstone-intro.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 12: Gesture Recognition for Human-Robot Interaction","permalink":"/physical-ai-robotics/docs/module-04-vla/week-12-gesture-recognition"},"next":{"title":"Week 13: Capstone Implementation - Complete System Integration","permalink":"/physical-ai-robotics/docs/module-04-vla/week-13-capstone-implementation"}}');var t=i(4848),r=i(8453);const l={},o="Week 13: Capstone Project - Autonomous Humanoid with Conversational AI",a={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"1. Voice Interface Layer",id:"1-voice-interface-layer",level:3},{value:"2. Planning and Reasoning Engine",id:"2-planning-and-reasoning-engine",level:3},{value:"3. Multimodal Perception",id:"3-multimodal-perception",level:3},{value:"4. Navigation and Manipulation",id:"4-navigation-and-manipulation",level:3},{value:"5. Feedback and Monitoring",id:"5-feedback-and-monitoring",level:3},{value:"System Data Flow",id:"system-data-flow",level:2},{value:"Example Interaction Scenario",id:"example-interaction-scenario",level:2},{value:"Technical Requirements",id:"technical-requirements",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software Stack",id:"software-stack",level:3},{value:"Development Environment",id:"development-environment",level:3},{value:"Project Milestones",id:"project-milestones",level:2},{value:"Milestone 1: Voice Interface (Week 13 Day 1-2)",id:"milestone-1-voice-interface-week-13-day-1-2",level:3},{value:"Milestone 2: Visual Grounding (Week 13 Day 3-4)",id:"milestone-2-visual-grounding-week-13-day-3-4",level:3},{value:"Milestone 3: Task Planning (Week 13 Day 5)",id:"milestone-3-task-planning-week-13-day-5",level:3},{value:"Milestone 4: Full Integration (Week 13 Day 6-7)",id:"milestone-4-full-integration-week-13-day-6-7",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Test Scenarios",id:"test-scenarios",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Resources",id:"resources",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-13-capstone-project---autonomous-humanoid-with-conversational-ai",children:"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Capstone Project Introduction",src:i(3596).A+"",width:"683",height:"1024"})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project synthesizes all skills from Module 4 into a fully integrated autonomous humanoid system. You will build a conversational AI agent that accepts natural language commands, plans multi-step tasks, perceives its environment through vision and language grounding, and executes physical actions\u2014all while providing natural language feedback to the user."}),"\n",(0,t.jsx)(n.p,{children:"This project represents the state-of-the-art in embodied AI: systems that seamlessly blend perception, reasoning, and action in human-centric environments. By completion, you will have hands-on experience with the same technologies powering commercial humanoid assistants like Figure 02, Tesla Optimus, and 1X NEO."}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The capstone integrates five core subsystems:"}),"\n",(0,t.jsx)(n.h3,{id:"1-voice-interface-layer",children:"1. Voice Interface Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper ASR"}),": Continuous speech recognition with wake-word detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Parser"}),": Extract intent, entities, and parameters from transcriptions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Text-to-Speech"}),": Generate natural language feedback (using ",(0,t.jsx)(n.code,{children:"pyttsx3"})," or OpenAI TTS)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-planning-and-reasoning-engine",children:"2. Planning and Reasoning Engine"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4 Task Planner"}),": Decompose high-level goals into action sequences"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ReAct Controller"}),": Adaptive plan execution with real-time replanning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory System"}),": Track world state, object locations, and task history"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-multimodal-perception",children:"3. Multimodal Perception"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLIP Visual Grounding"}),": Map language descriptions to scene objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": YOLOv8 or Detic for instance segmentation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Estimation"}),": RGB-D camera integration for 3D localization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gesture Recognition"}),": MediaPipe hand tracking for non-verbal commands"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-navigation-and-manipulation",children:"4. Navigation and Manipulation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nav2 Integration"}),": Autonomous navigation to target locations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoveIt 2"}),": Motion planning for arm manipulation tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Compute stable grasps from visual observations"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"5-feedback-and-monitoring",children:"5. Feedback and Monitoring"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Status"}),': Report task progress via speech ("Navigating to kitchen...")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Detect failures and request human assistance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Monitoring"}),": Collision avoidance, force limits, emergency stop"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-data-flow",children:"System Data Flow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"User Voice Command\r\n    \u2193\r\n[Whisper ASR] \u2192 Transcription\r\n    \u2193\r\n[GPT-4 Parser] \u2192 Structured Intent\r\n    \u2193\r\n[Task Planner] \u2192 Action Sequence\r\n    \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 ReAct Execution Loop:     \u2502\r\n\u2502                           \u2502\r\n\u2502 [Camera Feed]             \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 [CLIP + Object Detection] \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 [Visual Grounding]        \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 [Navigation/Manipulation] \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 [Execution Feedback]      \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 [Text-to-Speech Status]   \u2502\r\n\u2502      \u2193                    \u2502\r\n\u2502 Next Step or Complete     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"example-interaction-scenario",children:"Example Interaction Scenario"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"User"}),': "Hey robot, bring me the red mug from the kitchen counter."']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"System Processing"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice"}),": Whisper transcribes command"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parsing"}),": GPT-4 extracts intent=",(0,t.jsx)(n.code,{children:"fetch"}),", object=",(0,t.jsx)(n.code,{children:"red mug"}),", location=",(0,t.jsx)(n.code,{children:"kitchen counter"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning"}),": Generate steps:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Navigate to kitchen"}),"\n",(0,t.jsx)(n.li,{children:'Locate "red mug" using CLIP'}),"\n",(0,t.jsx)(n.li,{children:"Approach counter"}),"\n",(0,t.jsx)(n.li,{children:"Grasp mug"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to user"}),"\n",(0,t.jsx)(n.li,{children:"Hand over mug"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 1"}),': "Navigating to kitchen..." (TTS feedback)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 2"}),": Camera captures counter scene \u2192 CLIP identifies red mug at pixel (x, y) \u2192 Convert to 3D point"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 3"}),': "Approaching counter..." \u2192 Navigate to pre-grasp pose']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 4"}),": MoveIt plans grasp trajectory \u2192 Execute gripper closure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 5"}),': "Returning to you..." \u2192 Navigate back']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Step 6"}),': "Here is your red mug." \u2192 Extend arm for handover']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Adaptive Behavior"}),': If "red mug" not found on counter:']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Replan"}),': "I don\'t see a red mug on the counter. Should I check the dish rack?"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User"}),': "Yes, check the rack."']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Continue"}),": Update plan and search new location"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Humanoid robot platform (or simulated equivalent in Gazebo/Isaac Sim)"}),"\n",(0,t.jsx)(n.li,{children:"RGB-D camera (Intel RealSense D435, Kinect Azure)"}),"\n",(0,t.jsx)(n.li,{children:"Microphone and speakers for voice I/O"}),"\n",(0,t.jsx)(n.li,{children:"GPU with 8GB+ VRAM (NVIDIA RTX 3060 Ti or better)"}),"\n",(0,t.jsx)(n.li,{children:"Gripper with force/torque sensing"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"software-stack",children:"Software Stack"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Humble"})," on Ubuntu 22.04"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nav2"})," for autonomous navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoveIt 2"})," for manipulation planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI API"})," access (GPT-4, Whisper, TTS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python 3.10+"})," with PyTorch, transformers, OpenCV"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MediaPipe, CLIP, YOLOv8"})," for perception"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"development-environment",children:"Development Environment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install core dependencies\r\nsudo apt update\r\nsudo apt install -y ros-humble-nav2-* ros-humble-moveit python3-pip\r\n\r\n# Install Python packages\r\npip install openai whisper clip torch torchvision opencv-python mediapipe ultralytics pyttsx3\r\n\r\n# Clone capstone workspace\r\nmkdir -p ~/capstone_ws/src\r\ncd ~/capstone_ws/src\r\ngit clone <your_capstone_repo>\r\n\r\n# Build workspace\r\ncd ~/capstone_ws\r\ncolcon build --symlink-install\r\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(n.h2,{id:"project-milestones",children:"Project Milestones"}),"\n",(0,t.jsx)(n.h3,{id:"milestone-1-voice-interface-week-13-day-1-2",children:"Milestone 1: Voice Interface (Week 13 Day 1-2)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement wake-word activated listening"}),"\n",(0,t.jsx)(n.li,{children:"Integrate Whisper for transcription"}),"\n",(0,t.jsx)(n.li,{children:"Parse commands into structured intents"}),"\n",(0,t.jsx)(n.li,{children:"Add TTS feedback for acknowledgment"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),": System that accepts voice commands and confirms understanding"]}),"\n",(0,t.jsx)(n.h3,{id:"milestone-2-visual-grounding-week-13-day-3-4",children:"Milestone 2: Visual Grounding (Week 13 Day 3-4)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Set up camera feed in ROS 2"}),"\n",(0,t.jsx)(n.li,{children:"Integrate CLIP for object localization"}),"\n",(0,t.jsx)(n.li,{children:"Implement depth-based 3D coordinate conversion"}),"\n",(0,t.jsx)(n.li,{children:'Test referring expressions ("the leftmost mug")'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),": System that locates objects from natural language descriptions"]}),"\n",(0,t.jsx)(n.h3,{id:"milestone-3-task-planning-week-13-day-5",children:"Milestone 3: Task Planning (Week 13 Day 5)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement GPT-4 task decomposition"}),"\n",(0,t.jsx)(n.li,{children:"Build ReAct execution loop"}),"\n",(0,t.jsx)(n.li,{children:"Add world state tracking (object locations, robot pose)"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),": System that plans and executes multi-step tasks adaptively"]}),"\n",(0,t.jsx)(n.h3,{id:"milestone-4-full-integration-week-13-day-6-7",children:"Milestone 4: Full Integration (Week 13 Day 6-7)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Connect all subsystems (voice \u2192 planning \u2192 vision \u2192 action)"}),"\n",(0,t.jsx)(n.li,{children:"Implement error handling and recovery"}),"\n",(0,t.jsx)(n.li,{children:"Add gesture commands as alternative input"}),"\n",(0,t.jsx)(n.li,{children:"Deploy on physical robot or high-fidelity simulation"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),": End-to-end autonomous humanoid assistant"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,t.jsx)(n.p,{children:"Your capstone will be evaluated on:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Functionality (40%)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Correctly executes 5/5 test scenarios (fetch, place, navigate, search, collaborative task)"}),"\n",(0,t.jsx)(n.li,{children:"Handles edge cases (object not found, path blocked, grasp failure)"}),"\n",(0,t.jsx)(n.li,{children:"Adaptive replanning when initial plan fails"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Integration (25%)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Seamless coordination between voice, vision, planning, and control"}),"\n",(0,t.jsx)(n.li,{children:"Real-time performance (< 5s latency from command to action start)"}),"\n",(0,t.jsx)(n.li,{children:"Robust error handling across subsystems"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"User Experience (20%)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Natural language interaction (clear TTS feedback, confirmation loops)"}),"\n",(0,t.jsx)(n.li,{children:"Safe operation (collision avoidance, force limits)"}),"\n",(0,t.jsx)(n.li,{children:"Intuitive gesture commands"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Code Quality (15%)"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Modular architecture with clear interfaces"}),"\n",(0,t.jsx)(n.li,{children:"Comprehensive error handling"}),"\n",(0,t.jsx)(n.li,{children:"Inline documentation and comments"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,t.jsx)(n.p,{children:"Demonstrate your system on these tasks:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fetch and Deliver"}),': "Bring me the blue water bottle from the shelf"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Object Task"}),': "Clear the table by moving all mugs to the dish rack"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Search and Report"}),': "Is there a laptop on the desk? If so, what color is it?"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collaborative Cooking"}),': "Help me prepare dinner. First, get the cutting board, then the knife."']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gesture Override"}),": Use pointing gesture to indicate which object to grasp when multiple candidates exist"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.p,{children:["You've reviewed the capstone architecture and requirements. In ",(0,t.jsx)(n.strong,{children:"Week 13: Capstone Implementation"}),", you'll build the integrated system step-by-step, starting with the voice-to-action pipeline and progressing to full multimodal control."]}),"\n",(0,t.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Repository"}),": ",(0,t.jsx)(n.code,{children:"https://github.com/yourname/vla-capstone"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation"}),": System architecture diagrams, API references"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Support"}),": Office hours (link in course page), discussion forum"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Demo Videos"}),": Reference implementations for each milestone"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Ready to build your autonomous humanoid? Let's proceed to implementation."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);