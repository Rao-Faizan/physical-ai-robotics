"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[5849],{2071:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/ai-16-9e7d952177784162a04ea56d55dcfdb5.png"},8343:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-04-vla/week-13-capstone-implementation","title":"Week 13: Capstone Implementation - Complete System Integration","description":"Capstone Implementation","source":"@site/docs/module-04-vla/week-13-capstone-implementation.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-13-capstone-implementation","permalink":"/physical-ai-robotics/docs/module-04-vla/week-13-capstone-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-04-vla/week-13-capstone-implementation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI","permalink":"/physical-ai-robotics/docs/module-04-vla/week-13-capstone-intro"}}');var o=r(4848),a=r(8453);const i={},s="Week 13: Capstone Implementation - Complete System Integration",l={},c=[{value:"Step-by-Step Integration Guide",id:"step-by-step-integration-guide",level:2},{value:"Part 1: Voice-to-Intent Pipeline",id:"part-1-voice-to-intent-pipeline",level:2},{value:"Wake-Word Detection and Command Capture",id:"wake-word-detection-and-command-capture",level:3},{value:"Part 2: LLM Planning Integration",id:"part-2-llm-planning-integration",level:2},{value:"ReAct Task Executor with ROS 2",id:"react-task-executor-with-ros-2",level:3},{value:"Part 3: Text-to-Speech Feedback Node",id:"part-3-text-to-speech-feedback-node",level:2},{value:"Part 4: Launch File for Complete System",id:"part-4-launch-file-for-complete-system",level:2},{value:"Running the Complete System",id:"running-the-complete-system",level:2},{value:"Testing the System",id:"testing-the-system",level:2},{value:"Test 1: Simple Fetch Task",id:"test-1-simple-fetch-task",level:3},{value:"Test 2: Multi-Step Task",id:"test-2-multi-step-task",level:3},{value:"Test 3: Adaptive Replanning",id:"test-3-adaptive-replanning",level:3},{value:"Optimization and Debugging",id:"optimization-and-debugging",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"week-13-capstone-implementation---complete-system-integration",children:"Week 13: Capstone Implementation - Complete System Integration"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Capstone Implementation",src:r(2071).A+"",width:"736",height:"736"})}),"\n",(0,o.jsx)(n.h2,{id:"step-by-step-integration-guide",children:"Step-by-Step Integration Guide"}),"\n",(0,o.jsx)(n.p,{children:"This chapter provides a complete implementation of the VLA capstone system, integrating voice recognition, LLM planning, visual grounding, navigation, manipulation, and natural language feedback. Each component builds on previous modules to create a fully autonomous humanoid assistant."}),"\n",(0,o.jsx)(n.h2,{id:"part-1-voice-to-intent-pipeline",children:"Part 1: Voice-to-Intent Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"wake-word-detection-and-command-capture",children:"Wake-Word Detection and Command Capture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport sounddevice as sd\r\nimport numpy as np\r\nimport threading\r\nimport queue\r\n\r\nclass VoiceCommandNode(Node):\r\n    """\r\n    ROS 2 node for continuous voice command recognition.\r\n    Implements wake-word detection and Whisper transcription.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n\r\n        # Publisher for transcribed commands\r\n        self.command_pub = self.create_publisher(String, \'/voice_commands\', 10)\r\n\r\n        # Load Whisper model for speech recognition\r\n        self.get_logger().info("Loading Whisper model...")\r\n        self.whisper_model = whisper.load_model("base")\r\n\r\n        # Audio configuration\r\n        self.sample_rate = 16000\r\n        self.wake_word = "hey robot"\r\n        self.listening = False\r\n\r\n        # Audio queue for background recording\r\n        self.audio_queue = queue.Queue()\r\n\r\n        # Start audio capture thread\r\n        self.audio_thread = threading.Thread(target=self._audio_capture_loop, daemon=True)\r\n        self.audio_thread.start()\r\n\r\n        self.get_logger().info("Voice command system ready. Say \'Hey Robot\' to activate.")\r\n\r\n    def _audio_capture_loop(self):\r\n        """\r\n        Continuously capture audio and process for wake-word detection.\r\n        Runs in background thread to avoid blocking ROS callbacks.\r\n        """\r\n        while True:\r\n            # Record 3-second chunks\r\n            audio = sd.rec(\r\n                int(3 * self.sample_rate),\r\n                samplerate=self.sample_rate,\r\n                channels=1,\r\n                dtype=\'float32\'\r\n            )\r\n            sd.wait()\r\n\r\n            # Transcribe audio\r\n            audio_flat = audio.flatten()\r\n            result = self.whisper_model.transcribe(\r\n                audio_flat,\r\n                language=\'en\',\r\n                fp16=False  # CPU compatibility\r\n            )\r\n\r\n            transcription = result[\'text\'].lower().strip()\r\n            self.get_logger().debug(f"Heard: {transcription}")\r\n\r\n            # Check for wake-word\r\n            if self.wake_word in transcription:\r\n                self.get_logger().info("Wake-word detected! Listening for command...")\r\n                self._capture_command()\r\n\r\n    def _capture_command(self):\r\n        """\r\n        Record longer audio segment for command after wake-word.\r\n        """\r\n        # Record 5-second command\r\n        self.get_logger().info("Recording command...")\r\n        audio = sd.rec(\r\n            int(5 * self.sample_rate),\r\n            samplerate=self.sample_rate,\r\n            channels=1,\r\n            dtype=\'float32\'\r\n        )\r\n        sd.wait()\r\n\r\n        # Transcribe command\r\n        audio_flat = audio.flatten()\r\n        result = self.whisper_model.transcribe(audio_flat, language=\'en\', fp16=False)\r\n\r\n        command = result[\'text\'].strip()\r\n        self.get_logger().info(f"Command received: {command}")\r\n\r\n        # Publish command to planning system\r\n        msg = String()\r\n        msg.data = command\r\n        self.command_pub.publish(msg)\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = VoiceCommandNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"part-2-llm-planning-integration",children:"Part 2: LLM Planning Integration"}),"\n",(0,o.jsx)(n.h3,{id:"react-task-executor-with-ros-2",children:"ReAct Task Executor with ROS 2"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport os\r\nimport json\r\nfrom typing import Dict, List, Tuple\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom cv_bridge import CvBridge\r\n\r\nopenai.api_key = os.getenv("OPENAI_API_KEY")\r\n\r\nclass VLAExecutorNode(Node):\r\n    """\r\n    Main execution node integrating LLM planning with robot actions.\r\n    Implements ReAct pattern for adaptive task execution.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'vla_executor\')\r\n\r\n        # Subscribe to voice commands\r\n        self.cmd_sub = self.create_subscription(\r\n            String,\r\n            \'/voice_commands\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        # Subscribe to camera feed for visual grounding\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers for robot control\r\n        self.nav_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\r\n        self.grasp_pub = self.create_publisher(String, \'/grasp_command\', 10)\r\n        self.speech_pub = self.create_publisher(String, \'/tts_output\', 10)\r\n\r\n        # State tracking\r\n        self.current_image = None\r\n        self.bridge = CvBridge()\r\n        self.world_state = {\r\n            "robot_location": "living_room",\r\n            "held_object": None,\r\n            "known_objects": {}\r\n        }\r\n\r\n        self.execution_history = []\r\n        self.get_logger().info("VLA Executor ready.")\r\n\r\n    def image_callback(self, msg):\r\n        """Store latest camera image for visual grounding."""\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\r\n\r\n    def command_callback(self, msg):\r\n        """\r\n        Receive voice command and execute using ReAct loop.\r\n\r\n        Args:\r\n            msg: String message containing natural language command\r\n        """\r\n        command = msg.data\r\n        self.get_logger().info(f"Executing command: {command}")\r\n\r\n        # Generate initial plan with GPT-4\r\n        plan = self._generate_plan(command)\r\n\r\n        # Execute plan with ReAct loop\r\n        success = self._execute_react_loop(command, plan)\r\n\r\n        if success:\r\n            self._speak("Task completed successfully!")\r\n        else:\r\n            self._speak("I encountered a problem and couldn\'t complete the task.")\r\n\r\n    def _generate_plan(self, goal: str) -> List[Dict]:\r\n        """\r\n        Use GPT-4 to decompose goal into action sequence.\r\n\r\n        Returns:\r\n            List of action dictionaries with \'type\', \'parameters\'\r\n        """\r\n        system_prompt = """You are a task planner for a humanoid robot.\r\n        Generate step-by-step plans using these actions:\r\n\r\n        - navigate(location): Move to location\r\n        - locate(object_description): Find object using camera\r\n        - approach(object): Move close to object for manipulation\r\n        - grasp(object): Pick up object\r\n        - place(object, location): Put down object\r\n        - check(condition): Verify environment state\r\n\r\n        Return JSON array: [{"action": "type", "params": {...}}, ...]\r\n        """\r\n\r\n        user_prompt = f"""Goal: {goal}\r\n\r\n        Current state:\r\n        - Robot location: {self.world_state[\'robot_location\']}\r\n        - Held object: {self.world_state[\'held_object\']}\r\n\r\n        Generate action plan:\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {"role": "system", "content": system_prompt},\r\n                {"role": "user", "content": user_prompt}\r\n            ],\r\n            temperature=0.2\r\n        )\r\n\r\n        # Parse JSON plan\r\n        plan_text = response.choices[0].message.content\r\n        try:\r\n            # Extract JSON from response\r\n            start = plan_text.find(\'[\')\r\n            end = plan_text.rfind(\']\') + 1\r\n            plan = json.loads(plan_text[start:end])\r\n            return plan\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error("Failed to parse plan from LLM")\r\n            return []\r\n\r\n    def _execute_react_loop(self, goal: str, initial_plan: List[Dict]) -> bool:\r\n        """\r\n        Execute plan with adaptive replanning based on observations.\r\n\r\n        Args:\r\n            goal: Original user goal\r\n            initial_plan: Initial action sequence from LLM\r\n\r\n        Returns:\r\n            bool: True if goal achieved\r\n        """\r\n        plan = initial_plan\r\n        step = 0\r\n        max_steps = 20\r\n\r\n        while step < max_steps:\r\n            if not plan:\r\n                self.get_logger().info("Plan complete!")\r\n                return True\r\n\r\n            # Get next action\r\n            action = plan[0]\r\n            self.get_logger().info(f"Step {step}: {action}")\r\n\r\n            # Execute action and get observation\r\n            success, observation = self._execute_action(action)\r\n\r\n            # Update execution history\r\n            self.execution_history.append({\r\n                "step": step,\r\n                "action": action,\r\n                "success": success,\r\n                "observation": observation\r\n            })\r\n\r\n            if success:\r\n                # Remove completed action from plan\r\n                plan = plan[1:]\r\n                self._speak(f"Completed: {action[\'action\']}")\r\n            else:\r\n                # Replan on failure\r\n                self.get_logger().warn(f"Action failed: {observation}")\r\n                self._speak(f"Hmm, {observation}. Let me try a different approach.")\r\n\r\n                # Generate new plan from current state\r\n                plan = self._replan(goal, observation)\r\n\r\n                if not plan:\r\n                    self.get_logger().error("Unable to replan. Task failed.")\r\n                    return False\r\n\r\n            step += 1\r\n\r\n        self.get_logger().warn("Max steps reached without completion.")\r\n        return False\r\n\r\n    def _execute_action(self, action: Dict) -> Tuple[bool, str]:\r\n        """\r\n        Execute single action and return result.\r\n\r\n        Returns:\r\n            (success, observation): Execution result and observation\r\n        """\r\n        action_type = action[\'action\']\r\n        params = action.get(\'params\', {})\r\n\r\n        if action_type == \'navigate\':\r\n            return self._navigate(params[\'location\'])\r\n\r\n        elif action_type == \'locate\':\r\n            return self._locate_object(params[\'object_description\'])\r\n\r\n        elif action_type == \'approach\':\r\n            return self._approach_object(params[\'object\'])\r\n\r\n        elif action_type == \'grasp\':\r\n            return self._grasp_object(params[\'object\'])\r\n\r\n        elif action_type == \'place\':\r\n            return self._place_object(params[\'location\'])\r\n\r\n        elif action_type == \'check\':\r\n            return self._check_condition(params[\'condition\'])\r\n\r\n        else:\r\n            return False, f"Unknown action type: {action_type}"\r\n\r\n    def _navigate(self, location: str) -> Tuple[bool, str]:\r\n        """Navigate to location using Nav2."""\r\n        self.get_logger().info(f"Navigating to {location}...")\r\n\r\n        # In real implementation: publish nav goal and wait for result\r\n        # Mock success for demonstration\r\n        self.world_state[\'robot_location\'] = location\r\n        return True, f"Arrived at {location}"\r\n\r\n    def _locate_object(self, description: str) -> Tuple[bool, str]:\r\n        """\r\n        Use CLIP to find object in camera view.\r\n\r\n        Returns:\r\n            (success, observation): Whether found and location description\r\n        """\r\n        if self.current_image is None:\r\n            return False, "No camera image available"\r\n\r\n        # Use CLIP visual grounding (from Week 12)\r\n        # Simplified for demonstration\r\n        from PIL import Image as PILImage\r\n        import clip\r\n        import torch\r\n\r\n        device = "cuda" if torch.cuda.is_available() else "cpu"\r\n        model, preprocess = clip.load("ViT-B/32", device=device)\r\n\r\n        # Convert image\r\n        pil_image = PILImage.fromarray(self.current_image)\r\n        image_input = preprocess(pil_image).unsqueeze(0).to(device)\r\n\r\n        # Search for object\r\n        text_input = clip.tokenize([description, "empty scene"]).to(device)\r\n\r\n        with torch.no_grad():\r\n            image_features = model.encode_image(image_input)\r\n            text_features = model.encode_text(text_input)\r\n\r\n            image_features /= image_features.norm(dim=-1, keepdim=True)\r\n            text_features /= text_features.norm(dim=-1, keepdim=True)\r\n\r\n            similarity = (image_features @ text_features.T).squeeze(0)\r\n            probs = similarity.softmax(dim=0)\r\n\r\n        # Check if object detected with confidence\r\n        if probs[0] > 0.7:\r\n            # Store in world state\r\n            self.world_state[\'known_objects\'][description] = {\r\n                "location": self.world_state[\'robot_location\'],\r\n                "confidence": float(probs[0])\r\n            }\r\n            return True, f"Found {description} with confidence {probs[0]:.2f}"\r\n        else:\r\n            return False, f"Could not find {description} in view"\r\n\r\n    def _grasp_object(self, object_name: str) -> Tuple[bool, str]:\r\n        """Execute grasp using MoveIt2."""\r\n        self.get_logger().info(f"Grasping {object_name}...")\r\n\r\n        # Publish grasp command to manipulation controller\r\n        msg = String()\r\n        msg.data = f"grasp:{object_name}"\r\n        self.grasp_pub.publish(msg)\r\n\r\n        # Mock success\r\n        self.world_state[\'held_object\'] = object_name\r\n        return True, f"Successfully grasped {object_name}"\r\n\r\n    def _place_object(self, location: str) -> Tuple[bool, str]:\r\n        """Place held object at location."""\r\n        if self.world_state[\'held_object\'] is None:\r\n            return False, "Not holding any object"\r\n\r\n        object_name = self.world_state[\'held_object\']\r\n        self.get_logger().info(f"Placing {object_name} at {location}...")\r\n\r\n        # Publish place command\r\n        msg = String()\r\n        msg.data = f"place:{location}"\r\n        self.grasp_pub.publish(msg)\r\n\r\n        self.world_state[\'held_object\'] = None\r\n        return True, f"Placed {object_name} at {location}"\r\n\r\n    def _approach_object(self, object_name: str) -> Tuple[bool, str]:\r\n        """Move close to object for manipulation."""\r\n        if object_name not in self.world_state[\'known_objects\']:\r\n            return False, f"Location of {object_name} unknown"\r\n\r\n        # Navigate to object location\r\n        obj_info = self.world_state[\'known_objects\'][object_name]\r\n        return True, f"Approached {object_name}"\r\n\r\n    def _check_condition(self, condition: str) -> Tuple[bool, str]:\r\n        """Verify environmental condition using vision/sensors."""\r\n        # Use GPT-4V for visual question answering\r\n        # Simplified for demonstration\r\n        return True, f"Checked: {condition}"\r\n\r\n    def _replan(self, goal: str, failure_reason: str) -> List[Dict]:\r\n        """\r\n        Generate new plan after failure using failure observation.\r\n        """\r\n        prompt = f"""Original goal: {goal}\r\n\r\n        Previous plan failed because: {failure_reason}\r\n\r\n        Execution history:\r\n        {json.dumps(self.execution_history[-3:], indent=2)}\r\n\r\n        Current state:\r\n        {json.dumps(self.world_state, indent=2)}\r\n\r\n        Generate alternative plan as JSON array:\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.3\r\n        )\r\n\r\n        # Parse new plan\r\n        plan_text = response.choices[0].message.content\r\n        try:\r\n            start = plan_text.find(\'[\')\r\n            end = plan_text.rfind(\']\') + 1\r\n            new_plan = json.loads(plan_text[start:end])\r\n            return new_plan\r\n        except:\r\n            return []\r\n\r\n    def _speak(self, text: str):\r\n        """\r\n        Generate speech output via TTS.\r\n        """\r\n        self.get_logger().info(f"Speaking: {text}")\r\n\r\n        # Publish to TTS node\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_pub.publish(msg)\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = VLAExecutorNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"part-3-text-to-speech-feedback-node",children:"Part 3: Text-to-Speech Feedback Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyttsx3\r\n\r\nclass TTSNode(Node):\r\n    """\r\n    Text-to-Speech node for natural language feedback.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'tts_node\')\r\n\r\n        # Subscribe to speech requests\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/tts_output\',\r\n            self.speak_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize TTS engine\r\n        self.engine = pyttsx3.init()\r\n        self.engine.setProperty(\'rate\', 150)  # Words per minute\r\n        self.engine.setProperty(\'volume\', 0.9)\r\n\r\n        self.get_logger().info("TTS system ready.")\r\n\r\n    def speak_callback(self, msg):\r\n        """Convert text to speech."""\r\n        text = msg.data\r\n        self.get_logger().info(f"Speaking: {text}")\r\n\r\n        # Synthesize speech (non-blocking)\r\n        self.engine.say(text)\r\n        self.engine.runAndWait()\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = TTSNode()\r\n    rclpy.spin(node)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"part-4-launch-file-for-complete-system",children:"Part 4: Launch File for Complete System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# launch/vla_capstone.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Voice command node\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='voice_command_node',\r\n            name='voice_command',\r\n            output='screen'\r\n        ),\r\n\r\n        # VLA executor (planning + vision + control)\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='vla_executor_node',\r\n            name='vla_executor',\r\n            output='screen',\r\n            parameters=[{\r\n                'use_sim_time': False\r\n            }]\r\n        ),\r\n\r\n        # Text-to-speech feedback\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='tts_node',\r\n            name='tts',\r\n            output='screen'\r\n        ),\r\n\r\n        # Camera driver (replace with actual camera node)\r\n        Node(\r\n            package='usb_cam',\r\n            executable='usb_cam_node_exe',\r\n            name='camera',\r\n            parameters=[{\r\n                'video_device': '/dev/video0',\r\n                'framerate': 30.0,\r\n                'image_width': 640,\r\n                'image_height': 480\r\n            }]\r\n        )\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"running-the-complete-system",children:"Running the Complete System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch core robot systems (simulation or hardware)\r\nros2 launch your_robot_description robot.launch.py\r\n\r\n# Terminal 2: Launch VLA capstone system\r\nros2 launch vla_capstone vla_capstone.launch.py\r\n\r\n# Terminal 3: Monitor execution\r\nros2 topic echo /voice_commands\r\nros2 topic echo /tts_output\r\n\r\n# Terminal 4: Visualization\r\nrviz2 -d vla_capstone.rviz\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-the-system",children:"Testing the System"}),"\n",(0,o.jsx)(n.h3,{id:"test-1-simple-fetch-task",children:"Test 1: Simple Fetch Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Hey robot, bring me the red mug from the table."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'System confirms: "Navigating to table..."'}),"\n",(0,o.jsx)(n.li,{children:"Locates red mug using CLIP"}),"\n",(0,o.jsx)(n.li,{children:"Approaches and grasps mug"}),"\n",(0,o.jsx)(n.li,{children:'Returns: "Here is your red mug."'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"test-2-multi-step-task",children:"Test 2: Multi-Step Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Clear the desk by moving all mugs to the dish rack."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Navigates to desk"}),"\n",(0,o.jsx)(n.li,{children:"Iteratively locates mugs"}),"\n",(0,o.jsx)(n.li,{children:"For each mug: grasp \u2192 navigate to dish rack \u2192 place"}),"\n",(0,o.jsx)(n.li,{children:'Confirms: "Desk cleared. Moved 3 mugs to dish rack."'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"test-3-adaptive-replanning",children:"Test 3: Adaptive Replanning"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Get the laptop from the bedroom."']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Behavior"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Navigates to bedroom"}),"\n",(0,o.jsx)(n.li,{children:'If laptop not found: "I don\'t see a laptop in the bedroom. Should I check another room?"'}),"\n",(0,o.jsx)(n.li,{children:'User: "Try the living room."'}),"\n",(0,o.jsx)(n.li,{children:"Replans and searches living room"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"optimization-and-debugging",children:"Optimization and Debugging"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Tuning"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Use ",(0,o.jsx)(n.code,{children:"gpt-3.5-turbo"})," for faster planning (3x speedup over GPT-4)"]}),"\n",(0,o.jsx)(n.li,{children:"Cache CLIP embeddings for known objects"}),"\n",(0,o.jsx)(n.li,{children:"Run perception in separate thread to avoid blocking"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Common Issues"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper latency"}),": Use ",(0,o.jsx)(n.code,{children:"tiny"})," or ",(0,o.jsx)(n.code,{children:"base"})," model; consider distil-whisper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"CLIP false positives"}),": Increase confidence threshold to 0.8+"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan failures"}),": Add more few-shot examples to system prompt"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TTS blocking"}),": Run in separate process or use async synthesis"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"You have now built a complete Vision-Language-Action system integrating speech recognition, LLM planning, visual grounding, and robot control. This represents the current state-of-the-art in embodied AI, demonstrating how foundation models (GPT-4, CLIP, Whisper) can be orchestrated for real-world robotic applications."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next Steps"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Deploy on physical humanoid platform"}),"\n",(0,o.jsx)(n.li,{children:"Add more sophisticated error recovery (human-in-the-loop requests)"}),"\n",(0,o.jsx)(n.li,{children:"Implement memory persistence (remember object locations across sessions)"}),"\n",(0,o.jsx)(n.li,{children:"Integrate advanced manipulation (dexterous grasping, tool use)"}),"\n",(0,o.jsx)(n.li,{children:"Extend to multi-robot collaboration"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Congratulations on completing Module 4! You are now equipped to build next-generation autonomous humanoid robots with conversational AI capabilities."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>s});var t=r(6540);const o={},a=t.createContext(o);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);