"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[587],{6849:(e,n,r)=>{r.d(n,{A:()=>o});const o=r.p+"assets/images/ai-10-db73214c9e28195999797b0c7695e150.png"},8204:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"module-03-isaac/week-9-perception","title":"Week 9: Perception for Manipulation","description":"Perception for Manipulation","source":"@site/docs/module-03-isaac/week-9-perception.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-9-perception","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-9-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-03-isaac/week-9-perception.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 9: Isaac ROS Deployment","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-9-isaac-ros"},"next":{"title":"Week 10: Navigation with Nav2","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-10-nav2"}}');var t=r(4848),i=r(8453);const s={},a="Week 9: Perception for Manipulation",c={},p=[{value:"The Manipulation Perception Challenge",id:"the-manipulation-perception-challenge",level:2},{value:"Object Detection vs. Pose Estimation",id:"object-detection-vs-pose-estimation",level:2},{value:"2D Object Detection",id:"2d-object-detection",level:3},{value:"6-DOF Pose Estimation",id:"6-dof-pose-estimation",level:3},{value:"DOPE: Deep Object Pose Estimation",id:"dope-deep-object-pose-estimation",level:2},{value:"DOPE Architecture",id:"dope-architecture",level:3},{value:"Running Isaac ROS DOPE",id:"running-isaac-ros-dope",level:3},{value:"Training DOPE on Custom Objects",id:"training-dope-on-custom-objects",level:2},{value:"Step 1: Generate Synthetic Training Data",id:"step-1-generate-synthetic-training-data",level:3},{value:"Step 2: Train DOPE Model",id:"step-2-train-dope-model",level:3},{value:"Depth Perception for Manipulation",id:"depth-perception-for-manipulation",level:2},{value:"Integrating Depth with DOPE",id:"integrating-depth-with-dope",level:3},{value:"Grasp Planning with 6-DOF Poses",id:"grasp-planning-with-6-dof-poses",level:2},{value:"Exercises",id:"exercises",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-9-perception-for-manipulation",children:"Week 9: Perception for Manipulation"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Perception for Manipulation",src:r(6849).A+"",width:"736",height:"1104"})}),"\n",(0,t.jsx)(n.h2,{id:"the-manipulation-perception-challenge",children:"The Manipulation Perception Challenge"}),"\n",(0,t.jsxs)(n.p,{children:["Robotic manipulation requires knowing the ",(0,t.jsx)(n.strong,{children:"6-DOF pose"})," (3D position + 3D orientation) of objects with millimeter-level accuracy. A humanoid robot grasping a cup must know:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Position (x, y, z)"}),": Where is the cup's center?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Orientation (roll, pitch, yaw)"}),": How is the cup rotated?"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Traditional 2D object detection only provides bounding boxes, insufficient for 3D grasping."}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-vs-pose-estimation",children:"Object Detection vs. Pose Estimation"}),"\n",(0,t.jsx)(n.h3,{id:"2d-object-detection",children:"2D Object Detection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": [x, y, width, height, class, confidence]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Case"}),': "Is there a cup in the image?"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Limitation"}),": No depth or orientation information"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"6-dof-pose-estimation",children:"6-DOF Pose Estimation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": [x, y, z, qw, qx, qy, qz, class, confidence]"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Case"}),': "Where exactly is the cup in 3D space?"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Enables"}),": Grasp planning, collision avoidance, precise placement"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"dope-deep-object-pose-estimation",children:"DOPE: Deep Object Pose Estimation"}),"\n",(0,t.jsx)(n.p,{children:"DOPE (Deep Object Pose Estimation) is a CNN that predicts 6-DOF poses from RGB images. Trained on synthetic Isaac Sim data, DOPE achieves real-world accuracy through domain randomization."}),"\n",(0,t.jsx)(n.h3,{id:"dope-architecture",children:"DOPE Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Conceptual DOPE pipeline\r\ndef dope_inference(rgb_image):\r\n    # 1. Feature extraction (ResNet backbone)\r\n    features = resnet50(rgb_image)\r\n\r\n    # 2. Belief maps for 3D keypoints (e.g., object corners)\r\n    belief_maps = conv2d(features, num_keypoints=8)\r\n\r\n    # 3. PnP algorithm to recover 6-DOF pose from 2D keypoints\r\n    keypoints_2d = find_peaks(belief_maps)\r\n    pose_6dof = solve_pnp(keypoints_2d, keypoints_3d_model, camera_matrix)\r\n\r\n    return pose_6dof  # [x, y, z, quaternion]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"running-isaac-ros-dope",children:"Running Isaac ROS DOPE"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Launch file for DOPE object pose estimation\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\n\r\ndef generate_launch_description():\r\n    # Path to trained DOPE model for specific object\r\n    model_path = LaunchConfiguration('model_path', default='/workspaces/isaac_ros-dev/models/dope_soup_60.onnx')\r\n\r\n    return LaunchDescription([\r\n        # Camera input\r\n        Node(\r\n            package='realsense2_camera',\r\n            executable='realsense2_camera_node',\r\n            parameters=[{\r\n                'rgb_camera.profile': '640x480x30',\r\n                'depth_module.profile': '640x480x30',\r\n                'enable_depth': True,\r\n                'align_depth.enable': True  # Align depth to RGB\r\n            }]\r\n        ),\r\n\r\n        # DOPE inference node (TensorRT-accelerated)\r\n        Node(\r\n            package='isaac_ros_dope',\r\n            executable='dope_decoder',\r\n            name='dope',\r\n            parameters=[{\r\n                'object_name': 'soup_can',      # Object class this model detects\r\n                'model_file_path': model_path,\r\n                'configuration_file': '/workspaces/isaac_ros-dev/config/dope_config.yaml',\r\n                # Object dimensions in meters (for PnP)\r\n                'object_dimensions': [0.067, 0.067, 0.103],  # [width, depth, height]\r\n                # Detection threshold\r\n                'map_peak_threshold': 0.1\r\n            }],\r\n            remappings=[\r\n                ('image', '/camera/color/image_raw'),\r\n                ('camera_info', '/camera/color/camera_info'),\r\n                ('dope/pose_array', '/poses')  # Output: PoseArray message\r\n            ]\r\n        ),\r\n\r\n        # TF broadcaster to publish object pose in robot frame\r\n        Node(\r\n            package='isaac_ros_dope',\r\n            executable='dope_pose_to_tf',\r\n            parameters=[{\r\n                'object_frame_id': 'soup_can',\r\n                'reference_frame_id': 'camera_color_optical_frame'\r\n            }],\r\n            remappings=[\r\n                ('dope/pose_array', '/poses')\r\n            ]\r\n        ),\r\n\r\n        # Visualization\r\n        Node(\r\n            package='rviz2',\r\n            executable='rviz2',\r\n            arguments=['-d', './config/dope_visualization.rviz']\r\n        )\r\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"training-dope-on-custom-objects",children:"Training DOPE on Custom Objects"}),"\n",(0,t.jsx)(n.p,{children:"To detect your own objects, train DOPE using synthetic data from Isaac Sim:"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-generate-synthetic-training-data",children:"Step 1: Generate Synthetic Training Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\r\nfrom omni.isaac.kit import SimulationApp\r\n\r\nsimulation_app = SimulationApp({"headless": True})\r\n\r\n# Load your custom object (e.g., custom tool)\r\ncustom_object = rep.create.from_usd(\r\n    "./assets/custom_tool.usd",\r\n    semantics=[("class", "custom_tool")]\r\n)\r\n\r\ndef randomize_scene():\r\n    # Randomize object pose\r\n    with custom_object:\r\n        rep.modify.pose(\r\n            position=rep.distribution.uniform((-0.3, -0.3, 0.5), (0.3, 0.3, 0.8)),\r\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\r\n        )\r\n        # Randomize appearance\r\n        rep.randomizer.color(\r\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\r\n        )\r\n\r\n    # Randomize lighting\r\n    light = rep.create.light(\r\n        light_type="Dome",\r\n        intensity=rep.distribution.uniform(500, 2000)\r\n    )\r\n\r\n    return custom_object\r\n\r\nrep.randomizer.register(randomize_scene)\r\n\r\n# Camera for data capture\r\ncamera = rep.create.camera(\r\n    position=(1.0, 1.0, 0.6),\r\n    look_at=(0, 0, 0.5)\r\n)\r\n\r\n# DOPE requires RGB + keypoint annotations\r\nwriter = rep.WriterRegistry.get("BasicWriter")\r\nwriter.initialize(\r\n    output_dir="./dope_training_data",\r\n    rgb=True,\r\n    bounding_box_2d_tight=True,\r\n    bounding_box_3d=True  # 3D keypoints for DOPE training\r\n)\r\nwriter.attach([camera])\r\n\r\n# Generate 10,000 training images\r\nwith rep.trigger.on_frame(num_frames=10000):\r\n    rep.randomizer.randomize_scene()\r\n\r\nrep.orchestrator.run()\r\nsimulation_app.close()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-train-dope-model",children:"Step 2: Train DOPE Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Clone DOPE training repository\r\ngit clone https://github.com/NVlabs/Deep_Object_Pose.git\r\ncd Deep_Object_Pose\r\n\r\n# Prepare dataset (convert Isaac Sim output to DOPE format)\r\npython scripts/prepare_isaac_sim_data.py \\\r\n    --input_dir ./dope_training_data \\\r\n    --output_dir ./dope_formatted\r\n\r\n# Train DOPE network\r\npython train.py \\\r\n    --data ./dope_formatted \\\r\n    --object custom_tool \\\r\n    --epochs 60 \\\r\n    --batch_size 32 \\\r\n    --gpus 1\r\n\r\n# Export to ONNX for TensorRT optimization\r\npython export_onnx.py \\\r\n    --checkpoint ./checkpoints/custom_tool_epoch60.pth \\\r\n    --output custom_tool.onnx\n"})}),"\n",(0,t.jsx)(n.h2,{id:"depth-perception-for-manipulation",children:"Depth Perception for Manipulation"}),"\n",(0,t.jsx)(n.p,{children:"Depth sensors provide distance measurements critical for grasp planning."}),"\n",(0,t.jsx)(n.h3,{id:"integrating-depth-with-dope",children:"Integrating Depth with DOPE"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import PoseArray\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\n\r\nclass DepthEnhancedPose(Node):\r\n    def __init__(self):\r\n        super().__init__('depth_enhanced_pose')\r\n\r\n        # Subscribe to DOPE poses\r\n        self.pose_sub = self.create_subscription(\r\n            PoseArray,\r\n            '/poses',\r\n            self.pose_callback,\r\n            10\r\n        )\r\n\r\n        # Subscribe to depth image\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/aligned_depth_to_color/image_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n\r\n        self.bridge = CvBridge()\r\n        self.latest_depth = None\r\n\r\n    def depth_callback(self, msg):\r\n        # Convert ROS depth image to numpy array\r\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\r\n\r\n    def pose_callback(self, msg):\r\n        if self.latest_depth is None:\r\n            return\r\n\r\n        for pose in msg.poses:\r\n            # DOPE provides pose estimate, but depth can refine Z coordinate\r\n            # Project pose to image coordinates\r\n            u, v = self.project_to_image(pose.position)\r\n\r\n            # Look up depth at that pixel\r\n            if 0 <= u < self.latest_depth.shape[1] and 0 <= v < self.latest_depth.shape[0]:\r\n                measured_depth = self.latest_depth[v, u]\r\n\r\n                # Refine pose Z coordinate using depth sensor\r\n                refined_z = measured_depth * 0.001  # Convert mm to meters\r\n\r\n                self.get_logger().info(\r\n                    f\"DOPE Z: {pose.position.z:.3f}m, \"\r\n                    f\"Depth Z: {refined_z:.3f}m, \"\r\n                    f\"Error: {abs(pose.position.z - refined_z)*1000:.1f}mm\"\r\n                )\r\n\r\n    def project_to_image(self, position):\r\n        # Camera intrinsics (replace with actual values from camera_info)\r\n        fx, fy = 615.0, 615.0  # Focal lengths\r\n        cx, cy = 320.0, 240.0  # Principal point\r\n\r\n        # Project 3D point to 2D image\r\n        u = int(fx * position.x / position.z + cx)\r\n        v = int(fy * position.y / position.z + cy)\r\n        return u, v\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = DepthEnhancedPose()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"grasp-planning-with-6-dof-poses",children:"Grasp Planning with 6-DOF Poses"}),"\n",(0,t.jsx)(n.p,{children:"Once you have object poses, compute grasp configurations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom scipy.spatial.transform import Rotation\r\n\r\ndef plan_top_down_grasp(object_pose):\r\n    """\r\n    Plan a simple top-down grasp for a known object.\r\n\r\n    Args:\r\n        object_pose: geometry_msgs/Pose of the object\r\n\r\n    Returns:\r\n        grasp_pose: Desired end-effector pose for grasping\r\n    """\r\n    # Extract object position\r\n    obj_pos = np.array([\r\n        object_pose.position.x,\r\n        object_pose.position.y,\r\n        object_pose.position.z\r\n    ])\r\n\r\n    # Grasp approach: 10cm above object, gripper pointing down\r\n    grasp_offset = np.array([0, 0, 0.10])  # 10cm above\r\n    grasp_pos = obj_pos + grasp_offset\r\n\r\n    # Gripper orientation: Z-axis pointing down\r\n    grasp_rot = Rotation.from_euler(\'xyz\', [180, 0, 0], degrees=True)\r\n\r\n    # Align gripper with object orientation (for non-symmetric objects)\r\n    obj_rot = Rotation.from_quat([\r\n        object_pose.orientation.x,\r\n        object_pose.orientation.y,\r\n        object_pose.orientation.z,\r\n        object_pose.orientation.w\r\n    ])\r\n\r\n    # Combined rotation\r\n    final_rot = obj_rot * grasp_rot\r\n\r\n    return grasp_pos, final_rot.as_quat()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"DOPE Deployment"}),": Run Isaac ROS DOPE with a pre-trained model (soup can or cracker box) and visualize detections in RViz."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Depth Fusion"}),": Implement the DepthEnhancedPose node and compare DOPE Z estimates with depth sensor measurements."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Custom Object Detection"}),": Choose a household object, generate 1000 synthetic training images in Isaac Sim, and train a DOPE model."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Given a detected object pose, compute a grasp pose and publish it as a TF frame."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": ",(0,t.jsx)(n.a,{href:"/physical-ai-robotics/docs/module-03-isaac/week-10-nav2",children:"Week 10 - Navigation with Nav2"})]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var o=r(6540);const t={},i=o.createContext(t);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);