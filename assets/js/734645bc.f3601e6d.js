"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[4155],{5998:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-03-isaac/week-8-synthetic-data","title":"Week 8: Synthetic Data Generation","description":"The Synthetic Data Revolution","source":"@site/docs/module-03-isaac/week-8-synthetic-data.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-8-synthetic-data","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-8-synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-03-isaac/week-8-synthetic-data.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 8: Isaac Sim Fundamentals","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-8-isaac-sim"},"next":{"title":"Week 9: Isaac ROS Deployment","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-9-isaac-ros"}}');var i=r(4848),a=r(8453);const s={},o="Week 8: Synthetic Data Generation",c={},l=[{value:"The Synthetic Data Revolution",id:"the-synthetic-data-revolution",level:2},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Randomization Parameters",id:"randomization-parameters",level:3},{value:"Using Isaac Sim Replicator",id:"using-isaac-sim-replicator",level:2},{value:"Basic Synthetic Dataset Generation",id:"basic-synthetic-dataset-generation",level:3},{value:"Perception Ground Truth",id:"perception-ground-truth",level:2},{value:"1. 2D Bounding Boxes",id:"1-2d-bounding-boxes",level:3},{value:"2. Instance Segmentation",id:"2-instance-segmentation",level:3},{value:"3. Depth Maps",id:"3-depth-maps",level:3},{value:"4. 6-DOF Pose",id:"4-6-dof-pose",level:3},{value:"Creating a Custom Dataset for Humanoid Perception",id:"creating-a-custom-dataset-for-humanoid-perception",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-8-synthetic-data-generation",children:"Week 8: Synthetic Data Generation"})}),"\n",(0,i.jsx)(n.h2,{id:"the-synthetic-data-revolution",children:"The Synthetic Data Revolution"}),"\n",(0,i.jsx)(n.p,{children:"Manually labeling training data is expensive and time-consuming. A single annotated image for object detection can cost $0.50-$5.00. For a dataset of 100,000 images, this means $50,000-$500,000 in annotation costs."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Synthetic data generation"})," solves this by automatically creating labeled training data in simulation. Isaac Sim can generate thousands of perfectly labeled images per hour at near-zero marginal cost."]}),"\n",(0,i.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsxs)(n.p,{children:["The key to successful sim-to-real transfer is ",(0,i.jsx)(n.strong,{children:"domain randomization"})," - systematically varying scene parameters to create diversity that covers real-world variations."]}),"\n",(0,i.jsx)(n.h3,{id:"randomization-parameters",children:"Randomization Parameters"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lighting"}),": Intensity, color temperature, angle"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Textures"}),": Surface materials, colors, reflectivity"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object poses"}),": Position, rotation, scale"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Camera parameters"}),": FOV, exposure, focus"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Backgrounds"}),": Clutter, distractors, environments"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"using-isaac-sim-replicator",children:"Using Isaac Sim Replicator"}),"\n",(0,i.jsx)(n.p,{children:"Replicator is Isaac Sim's synthetic data generation framework, providing high-level APIs for creating randomized datasets."}),"\n",(0,i.jsx)(n.h3,{id:"basic-synthetic-dataset-generation",children:"Basic Synthetic Dataset Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\r\nfrom omni.isaac.kit import SimulationApp\r\n\r\n# Initialize headless simulation for faster generation\r\nsimulation_app = SimulationApp({"headless": True})\r\n\r\n# Define camera for capturing images\r\ncamera = rep.create.camera(\r\n    position=(2.0, 2.0, 1.5),  # Camera location\r\n    look_at=(0, 0, 0)           # Point camera at origin\r\n)\r\n\r\n# Create randomized scene\r\ndef create_scene():\r\n    # Random cube representing target object\r\n    cube = rep.create.cube(\r\n        position=rep.distribution.uniform((-1, -1, 0.5), (1, 1, 1.5)),\r\n        scale=rep.distribution.uniform(0.2, 0.5),\r\n        semantics=[("class", "target_object")]  # Label for detection\r\n    )\r\n\r\n    # Randomize cube material/color\r\n    with cube:\r\n        rep.randomizer.color(\r\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\r\n        )\r\n\r\n    # Random lighting direction and intensity\r\n    light = rep.create.light(\r\n        light_type="Distant",  # Directional light (sun-like)\r\n        intensity=rep.distribution.uniform(500, 3000),\r\n        rotation=rep.distribution.uniform((0, -180, 0), (0, 180, 0))\r\n    )\r\n\r\n    return cube, light\r\n\r\n# Register randomization graph\r\nrep.randomizer.register(create_scene)\r\n\r\n# Configure output writers for different annotation types\r\n# 1. RGB images\r\nrgb_writer = rep.WriterRegistry.get("BasicWriter")\r\nrgb_writer.initialize(\r\n    output_dir="./synthetic_data/rgb",\r\n    rgb=True\r\n)\r\n\r\n# 2. Semantic segmentation (pixel-wise class labels)\r\nsemantic_writer = rep.WriterRegistry.get("BasicWriter")\r\nsemantic_writer.initialize(\r\n    output_dir="./synthetic_data/semantic",\r\n    semantic_segmentation=True\r\n)\r\n\r\n# 3. Bounding boxes for object detection\r\nbbox_writer = rep.WriterRegistry.get("BasicWriter")\r\nbbox_writer.initialize(\r\n    output_dir="./synthetic_data/bbox",\r\n    bounding_box_2d_tight=True\r\n)\r\n\r\n# Attach all writers to camera\r\nrgb_writer.attach([camera])\r\nsemantic_writer.attach([camera])\r\nbbox_writer.attach([camera])\r\n\r\n# Generate 1000 randomized frames\r\nwith rep.trigger.on_frame(num_frames=1000):\r\n    rep.randomizer.create_scene()\r\n\r\n# Run orchestrator to execute generation\r\nrep.orchestrator.run()\r\n\r\nsimulation_app.close()\r\nprint("Generated 1000 synthetic training samples")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"perception-ground-truth",children:"Perception Ground Truth"}),"\n",(0,i.jsxs)(n.p,{children:["Isaac Sim automatically generates ",(0,i.jsx)(n.strong,{children:"perfect labels"})," for various perception tasks:"]}),"\n",(0,i.jsx)(n.h3,{id:"1-2d-bounding-boxes",children:"1. 2D Bounding Boxes"}),"\n",(0,i.jsx)(n.p,{children:"Pixel-perfect boxes around objects for object detection training (YOLO, Faster R-CNN)."}),"\n",(0,i.jsx)(n.h3,{id:"2-instance-segmentation",children:"2. Instance Segmentation"}),"\n",(0,i.jsx)(n.p,{children:"Pixel-wise masks identifying individual object instances."}),"\n",(0,i.jsx)(n.h3,{id:"3-depth-maps",children:"3. Depth Maps"}),"\n",(0,i.jsx)(n.p,{children:"Per-pixel distance from camera for stereo vision and 3D reconstruction."}),"\n",(0,i.jsx)(n.h3,{id:"4-6-dof-pose",children:"4. 6-DOF Pose"}),"\n",(0,i.jsx)(n.p,{children:"3D position (x, y, z) and orientation (roll, pitch, yaw) for each object."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Advanced ground truth generation\r\nimport omni.replicator.core as rep\r\n\r\ncamera = rep.create.camera()\r\n\r\n# Enable multiple annotation types simultaneously\r\nwriter = rep.WriterRegistry.get("BasicWriter")\r\nwriter.initialize(\r\n    output_dir="./multi_annotation",\r\n    rgb=True,                          # RGB images\r\n    bounding_box_2d_tight=True,        # 2D detection boxes\r\n    bounding_box_3d=True,              # 3D oriented boxes\r\n    semantic_segmentation=True,        # Pixel-wise class labels\r\n    instance_segmentation=True,        # Pixel-wise instance IDs\r\n    distance_to_camera=True,           # Depth map\r\n    distance_to_image_plane=True,      # Planar depth\r\n    normals=True,                      # Surface normals\r\n    motion_vectors=True                # Optical flow\r\n)\r\nwriter.attach([camera])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-custom-dataset-for-humanoid-perception",children:"Creating a Custom Dataset for Humanoid Perception"}),"\n",(0,i.jsx)(n.p,{children:"This example generates training data for detecting objects a humanoid robot might manipulate:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\r\n\r\n# Define objects humanoid should detect\r\ndef create_manipulation_scene():\r\n    # Ground plane with random texture\r\n    floor = rep.create.plane(\r\n        scale=10,\r\n        semantics=[("class", "floor")]\r\n    )\r\n    with floor:\r\n        rep.randomizer.texture(\r\n            textures=["./textures/wood.jpg", "./textures/tile.jpg"]\r\n        )\r\n\r\n    # Table at humanoid waist height\r\n    table = rep.create.cube(\r\n        position=(1.0, 0, 0.75),\r\n        scale=(1.0, 0.6, 0.05),\r\n        semantics=[("class", "table")]\r\n    )\r\n\r\n    # Randomized target objects on table\r\n    obj_types = ["cube", "sphere", "cylinder"]\r\n    for i in range(3):  # 3 random objects\r\n        obj = rep.create.from_usd(\r\n            f"./assets/{rep.distribution.choice(obj_types)}.usd",\r\n            position=rep.distribution.uniform(\r\n                (0.5, -0.3, 0.80),  # Table surface bounds\r\n                (1.5, 0.3, 0.80)\r\n            ),\r\n            semantics=[("class", "manipulable_object")]\r\n        )\r\n        with obj:\r\n            # Random materials make model robust to appearance\r\n            rep.randomizer.color(\r\n                colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\r\n            )\r\n\r\n    # Humanoid eye-level camera (160cm height)\r\n    camera = rep.create.camera(\r\n        position=(0, 0, 1.6),\r\n        rotation=rep.distribution.uniform((-10, -30, 0), (10, 30, 0))\r\n    )\r\n\r\n    return camera\r\n\r\nrep.randomizer.register(create_manipulation_scene)\r\n\r\n# Generate dataset with consistent format\r\nwriter = rep.WriterRegistry.get("BasicWriter")\r\nwriter.initialize(\r\n    output_dir="./humanoid_manipulation_dataset",\r\n    rgb=True,\r\n    bounding_box_2d_tight=True,\r\n    semantic_segmentation=True\r\n)\r\n\r\ncamera = create_manipulation_scene()\r\nwriter.attach([camera])\r\n\r\n# Generate 5000 training images\r\nwith rep.trigger.on_frame(num_frames=5000):\r\n    rep.randomizer.create_manipulation_scene()\r\n\r\nrep.orchestrator.run()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Match Real Sensors"}),": Configure camera resolution, FOV, and noise to match your physical robot's sensors."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sufficient Variation"}),": Generate at least 10x more synthetic data than you would collect manually."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validate Transfer"}),": Test trained models on small real-world validation sets to verify sim-to-real gap."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Incremental Complexity"}),": Start with simple scenes, add complexity as models improve."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Basic Randomization"}),": Create a dataset of 100 images with cubes of random colors and sizes at random positions."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Lighting Study"}),": Generate 50 images of the same scene with only lighting randomized. Observe how this affects object appearance."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Custom Objects"}),": Import a 3D model of an everyday object (cup, bottle) and generate a 1000-image detection dataset."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ground Truth Comparison"}),": Generate RGB + semantic segmentation pairs and visualize the segmentation masks overlaid on RGB images."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-robotics/docs/module-03-isaac/week-9-isaac-ros",children:"Week 9 - Isaac ROS Deployment"})]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var t=r(6540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);