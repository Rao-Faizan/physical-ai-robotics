"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[3096],{8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>o});var a=n(6540);const i={},s=a.createContext(i);function t(e){const r=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),a.createElement(s.Provider,{value:r},e.children)}},9730:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-03-isaac/week-9-isaac-ros","title":"Week 9: Isaac ROS Deployment","description":"Introduction to Isaac ROS","source":"@site/docs/module-03-isaac/week-9-isaac-ros.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/week-9-isaac-ros","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-9-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-03-isaac/week-9-isaac-ros.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 8: Synthetic Data Generation","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-8-synthetic-data"},"next":{"title":"Week 9: Perception for Manipulation","permalink":"/physical-ai-robotics/docs/module-03-isaac/week-9-perception"}}');var i=n(4848),s=n(8453);const t={},o="Week 9: Isaac ROS Deployment",c={},l=[{value:"Introduction to Isaac ROS",id:"introduction-to-isaac-ros",level:2},{value:"Why Hardware Acceleration Matters",id:"why-hardware-acceleration-matters",level:2},{value:"Isaac ROS Architecture",id:"isaac-ros-architecture",level:2},{value:"GEM (GPU-accelerated Module) Structure",id:"gem-gpu-accelerated-module-structure",level:3},{value:"Installation on Jetson Orin",id:"installation-on-jetson-orin",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installation Steps",id:"installation-steps",level:3},{value:"Hardware-Accelerated Visual SLAM",id:"hardware-accelerated-visual-slam",level:2},{value:"Traditional CPU VSLAM",id:"traditional-cpu-vslam",level:3},{value:"Isaac ROS VSLAM",id:"isaac-ros-vslam",level:3},{value:"Running Isaac ROS VSLAM",id:"running-isaac-ros-vslam",level:3},{value:"Stereo Vision for Depth Perception",id:"stereo-vision-for-depth-perception",level:2},{value:"Isaac ROS Stereo Disparity",id:"isaac-ros-stereo-disparity",level:3},{value:"DNN Inference with TensorRT",id:"dnn-inference-with-tensorrt",level:2},{value:"Converting PyTorch Model to TensorRT",id:"converting-pytorch-model-to-tensorrt",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"week-9-isaac-ros-deployment",children:"Week 9: Isaac ROS Deployment"})}),"\n",(0,i.jsx)(r.h2,{id:"introduction-to-isaac-ros",children:"Introduction to Isaac ROS"}),"\n",(0,i.jsxs)(r.p,{children:["Isaac ROS is a collection of ",(0,i.jsx)(r.strong,{children:"hardware-accelerated ROS 2 packages"})," (called GEMs) designed to run perception and navigation algorithms on NVIDIA GPUs. These packages leverage CUDA and TensorRT to achieve 10-100x speedups over CPU-based implementations."]}),"\n",(0,i.jsx)(r.h2,{id:"why-hardware-acceleration-matters",children:"Why Hardware Acceleration Matters"}),"\n",(0,i.jsx)(r.p,{children:"Humanoid robots require real-time perception for:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Balance control"}),": Processing IMU and vision data at 200+ Hz"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Obstacle avoidance"}),": Detecting objects in < 50ms for dynamic environments"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Manipulation"}),": 6-DOF pose estimation at 30 Hz for grasping moving objects"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:"CPU-based perception cannot meet these latency requirements. GPU acceleration makes real-time Physical AI feasible."}),"\n",(0,i.jsx)(r.h2,{id:"isaac-ros-architecture",children:"Isaac ROS Architecture"}),"\n",(0,i.jsx)(r.h3,{id:"gem-gpu-accelerated-module-structure",children:"GEM (GPU-accelerated Module) Structure"}),"\n",(0,i.jsx)(r.p,{children:"Each Isaac ROS GEM follows a standard pipeline:"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Input"}),": ROS 2 messages (images, point clouds, IMU data)"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"GPU Processing"}),": CUDA kernels for parallel computation"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Output"}),": ROS 2 messages (poses, detections, maps)"]}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# Conceptual GEM pipeline\r\ndef isaac_ros_gem(input_msg):\r\n    # Transfer data to GPU memory\r\n    gpu_data = cuda.to_device(input_msg.data)\r\n\r\n    # Parallel processing on thousands of CUDA cores\r\n    result = cuda_kernel_process(gpu_data)\r\n\r\n    # Transfer back to CPU for ROS publishing\r\n    output_msg = cuda.from_device(result)\r\n    return output_msg\n"})}),"\n",(0,i.jsx)(r.h2,{id:"installation-on-jetson-orin",children:"Installation on Jetson Orin"}),"\n",(0,i.jsx)(r.p,{children:"NVIDIA Jetson Orin is the recommended platform for deploying Isaac ROS on physical robots."}),"\n",(0,i.jsx)(r.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Hardware"}),": Jetson Orin Nano/NX/AGX (8GB+ RAM recommended)"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"JetPack"}),": 5.1 or later (includes CUDA, TensorRT, cuDNN)"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"ROS 2"}),": Humble Hawksbill"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"installation-steps",children:"Installation Steps"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Install Isaac ROS base dependencies\r\nsudo apt-get update\r\nsudo apt-get install -y \\\r\n    python3-pip \\\r\n    libopencv-dev \\\r\n    python3-opencv\r\n\r\n# Create workspace for Isaac ROS\r\nmkdir -p ~/isaac_ros_ws/src\r\ncd ~/isaac_ros_ws/src\r\n\r\n# Clone Isaac ROS common (required by all GEMs)\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\r\n\r\n# Clone specific GEMs (we'll use Visual SLAM)\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\r\n\r\n# Install dependencies using rosdep\r\ncd ~/isaac_ros_ws\r\nrosdep install --from-paths src --ignore-src -r -y\r\n\r\n# Build with colcon\r\ncolcon build --symlink-install\r\n\r\n# Source workspace\r\nsource ~/isaac_ros_ws/install/setup.bash\n"})}),"\n",(0,i.jsx)(r.h2,{id:"hardware-accelerated-visual-slam",children:"Hardware-Accelerated Visual SLAM"}),"\n",(0,i.jsx)(r.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) enables robots to build maps while tracking their position using only camera input."}),"\n",(0,i.jsx)(r.h3,{id:"traditional-cpu-vslam",children:"Traditional CPU VSLAM"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"ORB-SLAM3"}),": ~5-15 Hz on CPU, high latency"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"RTAB-Map"}),": ~10-20 Hz, struggles with high-resolution input"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"isaac-ros-vslam",children:"Isaac ROS VSLAM"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Performance"}),": 30-60 Hz on Jetson Orin"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Latency"}),": < 33ms (suitable for real-time control)"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Quality"}),": Leverages GPU for dense feature extraction"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"running-isaac-ros-vslam",children:"Running Isaac ROS VSLAM"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# Python launch file for Isaac ROS Visual SLAM\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Camera driver node (replace with your camera)\r\n        Node(\r\n            package='realsense2_camera',\r\n            executable='realsense2_camera_node',\r\n            name='camera',\r\n            parameters=[{\r\n                'enable_depth': True,\r\n                'depth_module.profile': '640x480x30',  # Resolution @ FPS\r\n                'rgb_camera.profile': '640x480x30'\r\n            }]\r\n        ),\r\n\r\n        # Isaac ROS Visual SLAM node\r\n        Node(\r\n            package='isaac_ros_visual_slam',\r\n            executable='isaac_ros_visual_slam',\r\n            name='visual_slam',\r\n            parameters=[{\r\n                'enable_imu': False,          # Set True if IMU available\r\n                'enable_rectified_pose': True, # Correct for camera distortion\r\n                'denoise_input_images': True,  # GPU-accelerated denoising\r\n                'rectified_images': True,\r\n                'enable_debug_mode': False,\r\n                'debug_dump_path': '/tmp/vslam_debug',\r\n                # Feature detection parameters\r\n                'num_cameras': 1,              # Single camera SLAM\r\n                'min_num_images': 10,          # Keyframes before map init\r\n                'horizontal_stereo_camera': False\r\n            }],\r\n            remappings=[\r\n                # Map camera topics to VSLAM inputs\r\n                ('stereo_camera/left/image', '/camera/color/image_raw'),\r\n                ('stereo_camera/left/camera_info', '/camera/color/camera_info'),\r\n                # VSLAM outputs\r\n                ('visual_slam/tracking/odometry', '/odom'),\r\n                ('visual_slam/tracking/vo_pose', '/vo_pose')\r\n            ]\r\n        ),\r\n\r\n        # Visualization in RViz\r\n        Node(\r\n            package='rviz2',\r\n            executable='rviz2',\r\n            name='rviz2',\r\n            arguments=['-d', './config/vslam.rviz']\r\n        )\r\n    ])\n"})}),"\n",(0,i.jsx)(r.h2,{id:"stereo-vision-for-depth-perception",children:"Stereo Vision for Depth Perception"}),"\n",(0,i.jsx)(r.p,{children:"Stereo cameras provide dense depth maps for obstacle avoidance and manipulation."}),"\n",(0,i.jsx)(r.h3,{id:"isaac-ros-stereo-disparity",children:"Isaac ROS Stereo Disparity"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Stereo camera driver\r\n        Node(\r\n            package='realsense2_camera',\r\n            executable='realsense2_camera_node',\r\n            parameters=[{\r\n                'enable_depth': False,  # Use stereo instead of active depth\r\n                'enable_infra1': True,  # Left IR camera\r\n                'enable_infra2': True,  # Right IR camera\r\n                'infra_width': 1280,\r\n                'infra_height': 720,\r\n                'infra_fps': 30\r\n            }]\r\n        ),\r\n\r\n        # Isaac ROS Stereo Disparity (GPU-accelerated)\r\n        Node(\r\n            package='isaac_ros_stereo_image_proc',\r\n            executable='disparity_node',\r\n            parameters=[{\r\n                'backends': 'CUDA',     # Force GPU execution\r\n                'max_disparity': 128.0, # Max depth range\r\n                'window_size': 5        # Matching window (odd number)\r\n            }],\r\n            remappings=[\r\n                ('left/image_rect', '/camera/infra1/image_rect_raw'),\r\n                ('left/camera_info', '/camera/infra1/camera_info'),\r\n                ('right/image_rect', '/camera/infra2/image_rect_raw'),\r\n                ('right/camera_info', '/camera/infra2/camera_info'),\r\n                ('disparity', '/disparity')\r\n            ]\r\n        ),\r\n\r\n        # Convert disparity to point cloud\r\n        Node(\r\n            package='isaac_ros_stereo_image_proc',\r\n            executable='point_cloud_node',\r\n            parameters=[{\r\n                'use_color': False  # Set True if RGB available\r\n            }],\r\n            remappings=[\r\n                ('disparity', '/disparity'),\r\n                ('left/camera_info', '/camera/infra1/camera_info'),\r\n                ('points2', '/points')\r\n            ]\r\n        )\r\n    ])\n"})}),"\n",(0,i.jsx)(r.h2,{id:"dnn-inference-with-tensorrt",children:"DNN Inference with TensorRT"}),"\n",(0,i.jsx)(r.p,{children:"Isaac ROS uses TensorRT to optimize neural networks for NVIDIA hardware."}),"\n",(0,i.jsx)(r.h3,{id:"converting-pytorch-model-to-tensorrt",children:"Converting PyTorch Model to TensorRT"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import torch\r\nimport torch.onnx\r\nfrom torch2trt import TRTModule\r\n\r\n# 1. Export PyTorch model to ONNX\r\nmodel = torch.load('object_detector.pth')\r\nmodel.eval()\r\n\r\ndummy_input = torch.randn(1, 3, 640, 640).cuda()  # Batch size 1, 3 channels, 640x640\r\ntorch.onnx.export(\r\n    model,\r\n    dummy_input,\r\n    \"object_detector.onnx\",\r\n    input_names=['input'],\r\n    output_names=['boxes', 'scores', 'classes'],\r\n    dynamic_axes={\r\n        'input': {0: 'batch_size'},  # Variable batch size\r\n        'boxes': {0: 'batch_size'},\r\n        'scores': {0: 'batch_size'}\r\n    }\r\n)\r\n\r\n# 2. Convert ONNX to TensorRT engine\r\nimport tensorrt as trt\r\n\r\nlogger = trt.Logger(trt.Logger.WARNING)\r\nbuilder = trt.Builder(logger)\r\nnetwork = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\r\nparser = trt.OnnxParser(network, logger)\r\n\r\n# Parse ONNX model\r\nwith open('object_detector.onnx', 'rb') as model_file:\r\n    if not parser.parse(model_file.read()):\r\n        for error in range(parser.num_errors):\r\n            print(parser.get_error(error))\r\n\r\n# Configure optimization (FP16 for Jetson)\r\nconfig = builder.create_builder_config()\r\nconfig.set_flag(trt.BuilderFlag.FP16)  # Half-precision for 2x speedup\r\nconfig.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\r\n\r\n# Build optimized engine\r\nserialized_engine = builder.build_serialized_network(network, config)\r\nwith open('object_detector.trt', 'wb') as f:\r\n    f.write(serialized_engine)\r\n\r\nprint(\"TensorRT engine saved - optimized for Jetson Orin\")\n"})}),"\n",(0,i.jsx)(r.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Algorithm"}),(0,i.jsx)(r.th,{children:"CPU (i7-12700)"}),(0,i.jsx)(r.th,{children:"GPU (RTX 3060)"}),(0,i.jsx)(r.th,{children:"Jetson Orin"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"VSLAM"}),(0,i.jsx)(r.td,{children:"12 Hz"}),(0,i.jsx)(r.td,{children:"90 Hz"}),(0,i.jsx)(r.td,{children:"45 Hz"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Stereo"}),(0,i.jsx)(r.td,{children:"8 Hz"}),(0,i.jsx)(r.td,{children:"120 Hz"}),(0,i.jsx)(r.td,{children:"60 Hz"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"YOLOv8"}),(0,i.jsx)(r.td,{children:"15 Hz"}),(0,i.jsx)(r.td,{children:"180 Hz"}),(0,i.jsx)(r.td,{children:"80 Hz"})]})]})]}),"\n",(0,i.jsx)(r.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"VSLAM Deployment"}),": Run Isaac ROS Visual SLAM with a webcam and observe pose estimates in RViz."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Stereo Depth"}),": If you have a stereo camera, generate a point cloud and visualize it. Otherwise, use recorded ROS bags from Isaac ROS examples."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"TensorRT Conversion"}),": Convert a simple PyTorch classifier to TensorRT and measure inference time before/after."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"GEM Integration"}),": Create a launch file that runs VSLAM + stereo disparity + object detection simultaneously."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Next"}),": ",(0,i.jsx)(r.a,{href:"/physical-ai-robotics/docs/module-03-isaac/week-9-perception",children:"Week 9 - Perception for Manipulation"})]})]})}function h(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);