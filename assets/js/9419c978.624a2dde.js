"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[3623],{4184:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-04-vla/week-12-gesture-recognition","title":"Week 12: Gesture Recognition for Human-Robot Interaction","description":"Gesture Recognition","source":"@site/docs/module-04-vla/week-12-gesture-recognition.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-12-gesture-recognition","permalink":"/physical-ai-robotics/docs/module-04-vla/week-12-gesture-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-04-vla/week-12-gesture-recognition.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 12: Multimodal Vision-Language Fusion","permalink":"/physical-ai-robotics/docs/module-04-vla/week-12-multimodal"},"next":{"title":"Week 13: Capstone Project - Autonomous Humanoid with Conversational AI","permalink":"/physical-ai-robotics/docs/module-04-vla/week-13-capstone-intro"}}');var t=r(4848),s=r(8453);const o={},a="Week 12: Gesture Recognition for Human-Robot Interaction",d={},l=[{value:"Hand Gesture Detection with MediaPipe",id:"hand-gesture-detection-with-mediapipe",level:2},{value:"Why MediaPipe for Robotics?",id:"why-mediapipe-for-robotics",level:3},{value:"Installation and Basic Hand Tracking",id:"installation-and-basic-hand-tracking",level:3},{value:"Gesture Command Mapping",id:"gesture-command-mapping",level:2},{value:"Pose Estimation for Human-Robot Interaction",id:"pose-estimation-for-human-robot-interaction",level:2},{value:"Full-Body Pose Tracking",id:"full-body-pose-tracking",level:3},{value:"Integrating Gestures with ROS 2",id:"integrating-gestures-with-ros-2",level:2},{value:"Practice Exercise",id:"practice-exercise",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"week-12-gesture-recognition-for-human-robot-interaction",children:"Week 12: Gesture Recognition for Human-Robot Interaction"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{alt:"Gesture Recognition",src:r(6485).A+"",width:"736",height:"736"})}),"\n",(0,t.jsx)(e.h2,{id:"hand-gesture-detection-with-mediapipe",children:"Hand Gesture Detection with MediaPipe"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"MediaPipe"})," is Google's open-source framework for real-time perception pipelines, offering pre-trained models for hand tracking, pose estimation, face detection, and holistic body tracking. For humanoid robotics, hand gesture recognition enables non-verbal communication\u2014pointing to objects, signaling stop/go, or demonstrating manipulation trajectories."]}),"\n",(0,t.jsx)(e.h3,{id:"why-mediapipe-for-robotics",children:"Why MediaPipe for Robotics?"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Real-Time Performance"}),": MediaPipe achieves 30+ FPS on CPU and 60+ FPS on GPU, suitable for responsive human-robot interaction without specialized hardware."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Cross-Platform"}),": Runs on Linux, Windows, mobile devices, and embedded systems (Raspberry Pi, Jetson Nano), enabling deployment on diverse robot platforms."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Pre-Trained Models"}),": MediaPipe's hand tracker identifies 21 3D landmarks per hand (fingertips, knuckles, wrist) without custom training\u2014no dataset annotation required."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Multi-Hand Support"}),": Detects and tracks multiple hands simultaneously, distinguishing left/right hand and maintaining identity across frames."]}),"\n",(0,t.jsx)(e.h3,{id:"installation-and-basic-hand-tracking",children:"Installation and Basic Hand Tracking"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Install MediaPipe and OpenCV for visualization\r\nimport subprocess\r\nsubprocess.run(["pip", "install", "mediapipe", "opencv-python", "numpy"])\r\n\r\nimport cv2\r\nimport mediapipe as mp\r\nimport numpy as np\r\n\r\n# Initialize MediaPipe hand tracking\r\nmp_hands = mp.solutions.hands\r\nmp_drawing = mp.solutions.drawing_utils\r\nmp_drawing_styles = mp.solutions.drawing_styles\r\n\r\ndef track_hands_in_video(video_source=0):\r\n    """\r\n    Real-time hand tracking from webcam or video file.\r\n\r\n    Args:\r\n        video_source: Camera index (0 for default webcam) or video file path\r\n    """\r\n    # Configure hand detector\r\n    # max_num_hands: Maximum hands to detect (1-2 for most robotics tasks)\r\n    # min_detection_confidence: Threshold for initial hand detection (0.0-1.0)\r\n    # min_tracking_confidence: Threshold for landmark tracking (0.0-1.0)\r\n    hands = mp_hands.Hands(\r\n        static_image_mode=False,  # False for video stream (enables tracking)\r\n        max_num_hands=2,\r\n        min_detection_confidence=0.7,  # Higher = fewer false positives\r\n        min_tracking_confidence=0.5\r\n    )\r\n\r\n    cap = cv2.VideoCapture(video_source)\r\n\r\n    while cap.isOpened():\r\n        success, frame = cap.read()\r\n        if not success:\r\n            break\r\n\r\n        # Convert BGR to RGB (MediaPipe expects RGB)\r\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n\r\n        # Process frame to detect hands\r\n        results = hands.process(frame_rgb)\r\n\r\n        # Draw hand landmarks on frame\r\n        if results.multi_hand_landmarks:\r\n            for hand_landmarks in results.multi_hand_landmarks:\r\n                # Draw 21 landmarks and connections\r\n                mp_drawing.draw_landmarks(\r\n                    frame,\r\n                    hand_landmarks,\r\n                    mp_hands.HAND_CONNECTIONS,\r\n                    mp_drawing_styles.get_default_hand_landmarks_style(),\r\n                    mp_drawing_styles.get_default_hand_connections_style()\r\n                )\r\n\r\n        # Display annotated frame\r\n        cv2.imshow(\'Hand Tracking\', frame)\r\n\r\n        if cv2.waitKey(5) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n    hands.close()\r\n\r\n# Run hand tracker\r\ntrack_hands_in_video()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"gesture-command-mapping",children:"Gesture Command Mapping"}),"\n",(0,t.jsx)(e.p,{children:"Map hand poses to robot commands by analyzing landmark configurations:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from typing import Optional, Dict\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass HandGesture:\r\n    """Represents a recognized gesture with confidence score."""\r\n    name: str\r\n    confidence: float\r\n    parameters: Dict  # Additional info (e.g., pointing direction)\r\n\r\nclass GestureRecognizer:\r\n    """\r\n    Classify hand gestures from MediaPipe landmarks.\r\n    Maps gestures to robot commands.\r\n    """\r\n\r\n    def __init__(self):\r\n        self.hands = mp_hands.Hands(\r\n            static_image_mode=False,\r\n            max_num_hands=1,  # Single hand for command gestures\r\n            min_detection_confidence=0.7,\r\n            min_tracking_confidence=0.5\r\n        )\r\n\r\n    def recognize_gesture(self, frame: np.ndarray) -> Optional[HandGesture]:\r\n        """\r\n        Detect and classify gesture in image frame.\r\n\r\n        Args:\r\n            frame: BGR image from camera\r\n\r\n        Returns:\r\n            HandGesture object or None if no gesture detected\r\n        """\r\n        # Process frame\r\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        results = self.hands.process(frame_rgb)\r\n\r\n        if not results.multi_hand_landmarks:\r\n            return None\r\n\r\n        # Analyze first detected hand\r\n        landmarks = results.multi_hand_landmarks[0]\r\n\r\n        # Convert landmarks to numpy array for easier computation\r\n        # Each landmark has x, y, z coordinates (normalized to [0, 1])\r\n        points = np.array([\r\n            [lm.x, lm.y, lm.z]\r\n            for lm in landmarks.landmark\r\n        ])\r\n\r\n        # Classify gesture based on landmark configuration\r\n        if self._is_thumbs_up(points):\r\n            return HandGesture("thumbs_up", 0.95, {})\r\n        elif self._is_pointing(points):\r\n            direction = self._get_pointing_direction(points)\r\n            return HandGesture("pointing", 0.90, {"direction": direction})\r\n        elif self._is_open_palm(points):\r\n            return HandGesture("stop", 0.92, {})\r\n        elif self._is_closed_fist(points):\r\n            return HandGesture("grab", 0.88, {})\r\n        elif self._is_peace_sign(points):\r\n            return HandGesture("peace", 0.85, {})\r\n\r\n        return HandGesture("unknown", 0.5, {})\r\n\r\n    def _is_thumbs_up(self, points: np.ndarray) -> bool:\r\n        """\r\n        Detect thumbs-up gesture.\r\n        Criteria: Thumb tip above MCP joint, other fingers curled.\r\n        """\r\n        # Landmark indices (see MediaPipe hand landmark diagram)\r\n        thumb_tip = points[4]\r\n        thumb_mcp = points[2]\r\n        index_tip = points[8]\r\n        index_pip = points[6]\r\n\r\n        # Thumb extended upward\r\n        thumb_extended = thumb_tip[1] < thumb_mcp[1]  # y decreases upward\r\n\r\n        # Other fingers curled (tip below PIP joint)\r\n        fingers_curled = all([\r\n            points[8][1] > points[6][1],   # Index\r\n            points[12][1] > points[10][1],  # Middle\r\n            points[16][1] > points[14][1],  # Ring\r\n            points[20][1] > points[18][1]   # Pinky\r\n        ])\r\n\r\n        return thumb_extended and fingers_curled\r\n\r\n    def _is_pointing(self, points: np.ndarray) -> bool:\r\n        """\r\n        Detect pointing gesture.\r\n        Criteria: Index finger extended, other fingers curled.\r\n        """\r\n        # Index finger extended\r\n        index_extended = points[8][1] < points[6][1]  # Tip above PIP\r\n\r\n        # Other fingers curled\r\n        other_curled = all([\r\n            points[12][1] > points[10][1],  # Middle\r\n            points[16][1] > points[14][1],  # Ring\r\n            points[20][1] > points[18][1]   # Pinky\r\n        ])\r\n\r\n        return index_extended and other_curled\r\n\r\n    def _is_open_palm(self, points: np.ndarray) -> bool:\r\n        """\r\n        Detect open palm (stop signal).\r\n        Criteria: All fingers extended and spread.\r\n        """\r\n        # All fingertips above their respective MCP joints\r\n        fingers_extended = all([\r\n            points[4][1] < points[2][1],    # Thumb\r\n            points[8][1] < points[5][1],    # Index\r\n            points[12][1] < points[9][1],   # Middle\r\n            points[16][1] < points[13][1],  # Ring\r\n            points[20][1] < points[17][1]   # Pinky\r\n        ])\r\n\r\n        return fingers_extended\r\n\r\n    def _is_closed_fist(self, points: np.ndarray) -> bool:\r\n        """\r\n        Detect closed fist (grab command).\r\n        Criteria: All fingers curled below MCP joints.\r\n        """\r\n        all_curled = all([\r\n            points[8][1] > points[5][1],    # Index\r\n            points[12][1] > points[9][1],   # Middle\r\n            points[16][1] > points[13][1],  # Ring\r\n            points[20][1] > points[17][1]   # Pinky\r\n        ])\r\n\r\n        return all_curled\r\n\r\n    def _is_peace_sign(self, points: np.ndarray) -> bool:\r\n        """\r\n        Detect peace sign (index + middle extended).\r\n        """\r\n        index_extended = points[8][1] < points[6][1]\r\n        middle_extended = points[12][1] < points[10][1]\r\n        ring_curled = points[16][1] > points[14][1]\r\n        pinky_curled = points[20][1] > points[18][1]\r\n\r\n        return index_extended and middle_extended and ring_curled and pinky_curled\r\n\r\n    def _get_pointing_direction(self, points: np.ndarray) -> str:\r\n        """\r\n        Compute pointing direction from index finger orientation.\r\n\r\n        Returns:\r\n            Direction string: "left", "right", "up", "down", "forward"\r\n        """\r\n        # Vector from index MCP to tip\r\n        mcp = points[5]\r\n        tip = points[8]\r\n        vector = tip - mcp\r\n\r\n        # Analyze primary component\r\n        if abs(vector[0]) > abs(vector[1]):\r\n            return "right" if vector[0] > 0 else "left"\r\n        else:\r\n            return "down" if vector[1] > 0 else "up"\r\n\r\n# Example: Real-time gesture recognition\r\nrecognizer = GestureRecognizer()\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile cap.isOpened():\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n\r\n    gesture = recognizer.recognize_gesture(frame)\r\n\r\n    if gesture and gesture.confidence > 0.8:\r\n        # Display recognized gesture\r\n        text = f"{gesture.name} ({gesture.confidence:.2f})"\r\n        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\r\n                    1, (0, 255, 0), 2)\r\n\r\n        # Execute robot command based on gesture\r\n        if gesture.name == "pointing":\r\n            print(f"Robot: Navigate {gesture.parameters[\'direction\']}")\r\n        elif gesture.name == "grab":\r\n            print("Robot: Execute grasp")\r\n        elif gesture.name == "stop":\r\n            print("Robot: Emergency stop")\r\n\r\n    cv2.imshow(\'Gesture Recognition\', frame)\r\n    if cv2.waitKey(5) & 0xFF == ord(\'q\'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"pose-estimation-for-human-robot-interaction",children:"Pose Estimation for Human-Robot Interaction"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Pose estimation"})," tracks full-body landmarks (33 points including shoulders, elbows, hips, knees) to understand human activity and spatial positioning. For collaborative robotics, this enables:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Proximity Detection"}),": Stop robot arm if human enters workspace"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Activity Recognition"}),": Distinguish standing, sitting, walking for context-aware behavior"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gesture Demonstration"}),": Learn manipulation tasks by imitating human arm movements"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"full-body-pose-tracking",children:"Full-Body Pose Tracking"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'mp_pose = mp.solutions.pose\r\n\r\ndef track_human_pose(video_source=0):\r\n    """\r\n    Track human body pose for safety monitoring and imitation learning.\r\n\r\n    Args:\r\n        video_source: Camera index or video file\r\n    """\r\n    pose = mp_pose.Pose(\r\n        static_image_mode=False,\r\n        model_complexity=1,  # 0=lite, 1=full, 2=heavy (higher = more accurate)\r\n        smooth_landmarks=True,  # Temporal smoothing for video\r\n        min_detection_confidence=0.5,\r\n        min_tracking_confidence=0.5\r\n    )\r\n\r\n    cap = cv2.VideoCapture(video_source)\r\n\r\n    while cap.isOpened():\r\n        success, frame = cap.read()\r\n        if not success:\r\n            break\r\n\r\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        results = pose.process(frame_rgb)\r\n\r\n        if results.pose_landmarks:\r\n            # Draw pose skeleton\r\n            mp_drawing.draw_landmarks(\r\n                frame,\r\n                results.pose_landmarks,\r\n                mp_pose.POSE_CONNECTIONS,\r\n                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\r\n            )\r\n\r\n            # Extract key joint positions for analysis\r\n            landmarks = results.pose_landmarks.landmark\r\n            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\r\n            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\r\n\r\n            # Example: Detect if hands are raised (y < 0.5)\r\n            hands_raised = (left_wrist.y < 0.5 and right_wrist.y < 0.5)\r\n\r\n            if hands_raised:\r\n                cv2.putText(frame, "Hands Raised - Robot Paused", (10, 60),\r\n                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\r\n\r\n        cv2.imshow(\'Pose Tracking\', frame)\r\n        if cv2.waitKey(5) & 0xFF == ord(\'q\'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n    pose.close()\r\n\r\n# Run pose tracker\r\ntrack_human_pose()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"integrating-gestures-with-ros-2",children:"Integrating Gestures with ROS 2"}),"\n",(0,t.jsx)(e.p,{children:"Connect gesture recognition to robot control:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass GestureControlNode(Node):\r\n    """\r\n    ROS 2 node that publishes robot commands based on hand gestures.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'gesture_control\')\r\n\r\n        # Publisher for gesture-based commands\r\n        self.cmd_pub = self.create_publisher(String, \'/gesture_commands\', 10)\r\n\r\n        # Timer for periodic gesture recognition\r\n        self.timer = self.create_timer(0.1, self.timer_callback)  # 10 Hz\r\n\r\n        self.recognizer = GestureRecognizer()\r\n        self.cap = cv2.VideoCapture(0)\r\n\r\n        self.last_gesture = None\r\n        self.gesture_stable_count = 0\r\n        self.STABILITY_THRESHOLD = 5  # Require 5 consecutive frames for confirmation\r\n\r\n    def timer_callback(self):\r\n        """Process camera frame and publish gesture commands."""\r\n        success, frame = self.cap.read()\r\n        if not success:\r\n            return\r\n\r\n        gesture = self.recognizer.recognize_gesture(frame)\r\n\r\n        if gesture and gesture.confidence > 0.85:\r\n            # Require gesture stability to avoid false triggers\r\n            if gesture.name == self.last_gesture:\r\n                self.gesture_stable_count += 1\r\n            else:\r\n                self.gesture_stable_count = 0\r\n                self.last_gesture = gesture.name\r\n\r\n            # Publish command if gesture is stable\r\n            if self.gesture_stable_count >= self.STABILITY_THRESHOLD:\r\n                msg = String()\r\n                msg.data = f"{gesture.name}:{gesture.parameters}"\r\n                self.cmd_pub.publish(msg)\r\n\r\n                self.get_logger().info(f"Published gesture command: {gesture.name}")\r\n\r\n                # Reset to avoid repeated commands\r\n                self.gesture_stable_count = 0\r\n\r\ndef main():\r\n    rclpy.init()\r\n    node = GestureControlNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,t.jsx)(e.p,{children:"Build a gesture-controlled navigation system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement pointing direction detection (8 directions: N, NE, E, SE, S, SW, W, NW)"}),"\n",(0,t.jsxs)(e.li,{children:["Map gestures to navigation commands:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Point \u2192 Navigate in indicated direction"}),"\n",(0,t.jsx)(e.li,{children:"Open palm \u2192 Stop"}),"\n",(0,t.jsx)(e.li,{children:"Closed fist \u2192 Return to home position"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.li,{children:"Add gesture confirmation: Require 1-second hold before executing command"}),"\n",(0,t.jsx)(e.li,{children:"Implement safety checks: Stop robot if human enters proximity (use pose estimation)"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.p,{children:["You've completed multimodal perception (voice, vision, gestures). In ",(0,t.jsx)(e.strong,{children:"Week 13: Capstone Project"}),", you'll integrate all components into an autonomous humanoid system with conversational AI, visual grounding, and adaptive task execution."]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},6485:(n,e,r)=>{r.d(e,{A:()=>i});const i=r.p+"assets/images/ai-14-4affdfa303c322bf150f9819dc34a14e.png"},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>a});var i=r(6540);const t={},s=i.createContext(t);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);