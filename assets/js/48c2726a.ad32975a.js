"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[7324],{3763:(n,e,r)=>{r.d(e,{A:()=>t});const t=r.p+"assets/images/ai-12-9b64b408e75756347a6980d036ec6d0f.png"},5833:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-04-vla/week-11-voice-to-action","title":"Week 11: Voice-to-Action Pipeline","description":"Voice-to-Action Pipeline","source":"@site/docs/module-04-vla/week-11-voice-to-action.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-11-voice-to-action","permalink":"/physical-ai-robotics/docs/module-04-vla/week-11-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Rao-Faizan/physical-ai-robotics/tree/main/frontend/docs/module-04-vla/week-11-voice-to-action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action Integration","permalink":"/physical-ai-robotics/docs/module-04-vla/intro"},"next":{"title":"Week 11: LLM-Based Task Planning","permalink":"/physical-ai-robotics/docs/module-04-vla/week-11-llm-planning"}}');var i=r(4848),o=r(8453);const a={},s="Week 11: Voice-to-Action Pipeline",c={},l=[{value:"Speech Recognition with OpenAI Whisper",id:"speech-recognition-with-openai-whisper",level:2},{value:"Why Whisper for Robotics?",id:"why-whisper-for-robotics",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Command Parsing and Intent Classification",id:"command-parsing-and-intent-classification",level:2},{value:"Rule-Based Parsing",id:"rule-based-parsing",level:3},{value:"LLM-Based Intent Classification",id:"llm-based-intent-classification",level:3},{value:"Voice Interface Design",id:"voice-interface-design",level:2},{value:"Practice Exercise",id:"practice-exercise",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"week-11-voice-to-action-pipeline",children:"Week 11: Voice-to-Action Pipeline"})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.img,{alt:"Voice-to-Action Pipeline",src:r(3763).A+"",width:"736",height:"1104"})}),"\n",(0,i.jsx)(e.h2,{id:"speech-recognition-with-openai-whisper",children:"Speech Recognition with OpenAI Whisper"}),"\n",(0,i.jsx)(e.p,{children:"OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) system trained on 680,000 hours of multilingual data. Unlike traditional ASR models requiring custom wake-word detection and language-specific training, Whisper provides zero-shot transcription across 99 languages with robust performance in noisy environments\u2014critical for humanoid robots operating in homes, factories, and public spaces."}),"\n",(0,i.jsx)(e.h3,{id:"why-whisper-for-robotics",children:"Why Whisper for Robotics?"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Noise Robustness"}),": Whisper's training included diverse acoustic conditions (background music, overlapping speech, machinery noise). A humanoid working in a kitchen can transcribe commands over running dishwashers and conversations."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Multilingual Support"}),": Global deployments require multi-language interfaces. Whisper handles code-switching (mixing languages mid-sentence) common in multilingual households."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"No Fine-Tuning Required"}),': Unlike domain-specific ASR models, Whisper generalizes to robotics vocabulary ("grasp the Phillips screwdriver") without custom training.']}),"\n",(0,i.jsx)(e.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Install Whisper and dependencies\r\n# Requires ffmpeg for audio processing: sudo apt install ffmpeg\r\nimport subprocess\r\nsubprocess.run(["pip", "install", "openai-whisper", "sounddevice", "numpy"])\r\n\r\nimport whisper\r\nimport sounddevice as sd\r\nimport numpy as np\r\nfrom scipy.io.wavfile import write\r\n\r\n# Load Whisper model (options: tiny, base, small, medium, large)\r\n# Trade-off: larger models = better accuracy but slower inference\r\n# For real-time robotics: \'base\' (74M params) achieves <0.5s latency on GPU\r\nmodel = whisper.load_model("base")  # Downloads ~140MB on first run\r\n\r\ndef record_audio(duration=5, sample_rate=16000):\r\n    """\r\n    Record audio from microphone for specified duration.\r\n\r\n    Args:\r\n        duration: Recording length in seconds\r\n        sample_rate: Hz (16kHz is Whisper\'s native rate, avoids resampling)\r\n\r\n    Returns:\r\n        numpy array: Audio samples in range [-1, 1]\r\n    """\r\n    print(f"Recording for {duration} seconds...")\r\n    # Record from default microphone (set device index for specific mic)\r\n    audio = sd.rec(\r\n        int(duration * sample_rate),\r\n        samplerate=sample_rate,\r\n        channels=1,  # Mono audio\r\n        dtype=\'float32\'\r\n    )\r\n    sd.wait()  # Block until recording completes\r\n    print("Recording complete.")\r\n    return audio.flatten()\r\n\r\ndef transcribe_audio(audio_array):\r\n    """\r\n    Transcribe audio to text using Whisper.\r\n\r\n    Args:\r\n        audio_array: NumPy array of audio samples\r\n\r\n    Returns:\r\n        dict: Transcription result with \'text\', \'language\', \'segments\'\r\n    """\r\n    # Whisper expects float32 audio normalized to [-1, 1]\r\n    result = model.transcribe(\r\n        audio_array,\r\n        language=\'en\',  # Set to None for auto-detection (adds latency)\r\n        task=\'transcribe\',  # Alternative: \'translate\' for non-English to English\r\n        fp16=True  # Enable half-precision for 2x speed on GPU\r\n    )\r\n    return result\r\n\r\n# Example usage: Record and transcribe\r\naudio = record_audio(duration=5)\r\nresult = transcribe_audio(audio)\r\nprint(f"Transcription: {result[\'text\']}")\r\n# Output example: "Robot, pick up the red mug and place it on the table."\n'})}),"\n",(0,i.jsx)(e.h2,{id:"command-parsing-and-intent-classification",children:"Command Parsing and Intent Classification"}),"\n",(0,i.jsxs)(e.p,{children:["Raw transcriptions require parsing into structured robot commands. We extract ",(0,i.jsx)(e.strong,{children:"intent"})," (what action), ",(0,i.jsx)(e.strong,{children:"entities"})," (target objects), and ",(0,i.jsx)(e.strong,{children:"parameters"})," (locations, quantities)."]}),"\n",(0,i.jsx)(e.h3,{id:"rule-based-parsing",children:"Rule-Based Parsing"}),"\n",(0,i.jsx)(e.p,{children:"For constrained vocabularies (warehouse robots with fixed commands), rule-based parsing suffices:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import re\r\nfrom typing import Dict, List, Optional\r\n\r\nclass CommandParser:\r\n    \"\"\"\r\n    Parse natural language into structured robot commands.\r\n    Handles imperative sentences with action verbs and object references.\r\n    \"\"\"\r\n\r\n    # Define action vocabulary with synonyms\r\n    ACTION_VERBS = {\r\n        'pick': ['pick', 'grab', 'grasp', 'take', 'lift'],\r\n        'place': ['place', 'put', 'set', 'drop', 'position'],\r\n        'navigate': ['go', 'move', 'walk', 'navigate', 'travel'],\r\n        'open': ['open'],\r\n        'close': ['close', 'shut']\r\n    }\r\n\r\n    # Spatial prepositions for location extraction\r\n    LOCATIONS = ['on', 'in', 'under', 'next to', 'above', 'below', 'near']\r\n\r\n    def __init__(self):\r\n        # Compile regex patterns for efficiency\r\n        self.action_pattern = self._build_action_pattern()\r\n\r\n    def _build_action_pattern(self) -> re.Pattern:\r\n        \"\"\"Create regex matching any action verb.\"\"\"\r\n        all_verbs = [v for synonyms in self.ACTION_VERBS.values() for v in synonyms]\r\n        pattern = r'\\b(' + '|'.join(all_verbs) + r')\\b'\r\n        return re.compile(pattern, re.IGNORECASE)\r\n\r\n    def parse(self, text: str) -> Dict:\r\n        \"\"\"\r\n        Extract intent and entities from command.\r\n\r\n        Args:\r\n            text: Natural language command\r\n\r\n        Returns:\r\n            dict: {\r\n                'intent': str,\r\n                'object': str,\r\n                'location': str,\r\n                'confidence': float\r\n            }\r\n        \"\"\"\r\n        text = text.lower().strip()\r\n\r\n        # Extract action intent\r\n        action_match = self.action_pattern.search(text)\r\n        if not action_match:\r\n            return {'intent': 'unknown', 'confidence': 0.0}\r\n\r\n        verb = action_match.group(1)\r\n        intent = self._map_verb_to_intent(verb)\r\n\r\n        # Extract target object (noun after action verb)\r\n        object_match = re.search(\r\n            r'\\b(?:the\\s+)?(\\w+(?:\\s+\\w+)?)\\b',  # Captures \"red mug\" or \"mug\"\r\n            text[action_match.end():]\r\n        )\r\n        target_object = object_match.group(1) if object_match else None\r\n\r\n        # Extract location (prepositional phrase)\r\n        location = self._extract_location(text)\r\n\r\n        return {\r\n            'intent': intent,\r\n            'object': target_object,\r\n            'location': location,\r\n            'confidence': 0.95 if target_object else 0.6\r\n        }\r\n\r\n    def _map_verb_to_intent(self, verb: str) -> str:\r\n        \"\"\"Map detected verb to canonical intent.\"\"\"\r\n        for intent, synonyms in self.ACTION_VERBS.items():\r\n            if verb in synonyms:\r\n                return intent\r\n        return 'unknown'\r\n\r\n    def _extract_location(self, text: str) -> Optional[str]:\r\n        \"\"\"Extract location phrase (e.g., 'on the table').\"\"\"\r\n        for prep in self.LOCATIONS:\r\n            pattern = f'{prep}\\\\s+(?:the\\\\s+)?(\\\\w+(?:\\\\s+\\\\w+)?)'\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                return f\"{prep} {match.group(1)}\"\r\n        return None\r\n\r\n# Example usage\r\nparser = CommandParser()\r\ncommands = [\r\n    \"Pick up the red mug\",\r\n    \"Place it on the table\",\r\n    \"Go to the kitchen\",\r\n    \"Open the drawer\"\r\n]\r\n\r\nfor cmd in commands:\r\n    result = parser.parse(cmd)\r\n    print(f\"Input: {cmd}\")\r\n    print(f\"Parsed: {result}\\n\")\r\n\r\n# Output:\r\n# Input: Pick up the red mug\r\n# Parsed: {'intent': 'pick', 'object': 'red mug', 'location': None, 'confidence': 0.95}\n"})}),"\n",(0,i.jsx)(e.h3,{id:"llm-based-intent-classification",children:"LLM-Based Intent Classification"}),"\n",(0,i.jsx)(e.p,{children:"For open-ended commands, use GPT-4 for semantic understanding:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport os\r\nimport json\r\n\r\n# Set API key from environment variable (never hardcode secrets)\r\nopenai.api_key = os.getenv("OPENAI_API_KEY")\r\n\r\ndef classify_intent_with_llm(command: str) -> Dict:\r\n    """\r\n    Use GPT-4 to extract structured intent from natural language.\r\n    Handles complex commands like \'After closing the door, bring me water.\'\r\n\r\n    Args:\r\n        command: Natural language instruction\r\n\r\n    Returns:\r\n        dict: Structured command with intent, parameters, and sequence\r\n    """\r\n    # System prompt defines robot\'s capabilities and output format\r\n    system_prompt = """You are a command parser for a humanoid robot.\r\n    Extract intent, objects, locations, and action sequences from user commands.\r\n\r\n    Available actions: pick, place, navigate, open, close, wait.\r\n\r\n    Return JSON with format:\r\n    {\r\n        "actions": [\r\n            {"intent": "action", "object": "item", "location": "place", "parameters": {}}\r\n        ],\r\n        "confidence": 0.0-1.0\r\n    }\r\n    """\r\n\r\n    # Few-shot examples improve parsing accuracy\r\n    user_prompt = f"""Command: {command}\r\n\r\n    Examples:\r\n    Input: "Grab the blue cup and put it in the sink"\r\n    Output: {{"actions": [{{"intent": "pick", "object": "blue cup"}}, {{"intent": "place", "location": "sink"}}], "confidence": 0.95}}\r\n\r\n    Input: "Go to the bedroom"\r\n    Output: {{"actions": [{{"intent": "navigate", "location": "bedroom"}}], "confidence": 0.98}}\r\n\r\n    Now parse the command above.\r\n    """\r\n\r\n    response = openai.ChatCompletion.create(\r\n        model="gpt-4",  # Use gpt-3.5-turbo for faster/cheaper inference\r\n        messages=[\r\n            {"role": "system", "content": system_prompt},\r\n            {"role": "user", "content": user_prompt}\r\n        ],\r\n        temperature=0.0,  # Deterministic output for command parsing\r\n        max_tokens=200\r\n    )\r\n\r\n    # Extract JSON from response\r\n    result_text = response.choices[0].message.content\r\n    try:\r\n        return json.loads(result_text)\r\n    except json.JSONDecodeError:\r\n        return {"actions": [], "confidence": 0.0, "error": "Parse failed"}\r\n\r\n# Example: Complex multi-step command\r\ncommand = "First go to the kitchen, then pick up the green bottle and bring it here."\r\nintent = classify_intent_with_llm(command)\r\nprint(json.dumps(intent, indent=2))\r\n\r\n# Output:\r\n# {\r\n#   "actions": [\r\n#     {"intent": "navigate", "location": "kitchen"},\r\n#     {"intent": "pick", "object": "green bottle"},\r\n#     {"intent": "navigate", "location": "user"}\r\n#   ],\r\n#   "confidence": 0.92\r\n# }\n'})}),"\n",(0,i.jsx)(e.h2,{id:"voice-interface-design",children:"Voice Interface Design"}),"\n",(0,i.jsx)(e.p,{children:"Effective human-robot voice interaction requires:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Wake Word Detection"}),': Use lightweight models (Porcupine, Snowboy) to activate listening only on "Hey Robot", reducing false activations.']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Confirmation Loops"}),': Repeat parsed commands back to user: "I will pick up the red mug. Proceed?" Prevents misunderstandings before execution.']}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Feedback"}),': Provide audio acknowledgments during long operations: "Navigating to kitchen... Arrived. Searching for green bottle..."']}),"\n",(0,i.jsx)(e.h3,{id:"practice-exercise",children:"Practice Exercise"}),"\n",(0,i.jsx)(e.p,{children:"Implement a complete voice command pipeline:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Record 5-second audio clip"}),"\n",(0,i.jsx)(e.li,{children:"Transcribe with Whisper"}),"\n",(0,i.jsx)(e.li,{children:"Parse intent with rule-based parser"}),"\n",(0,i.jsx)(e.li,{children:"Generate ROS 2 action goal for robot execution"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:'Extend the parser to handle negations ("Don\'t pick up the blue cup") and conditionals ("If the door is open, go through it").'}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(e.p,{children:["You've built the perception layer (speech \u2192 text \u2192 intent). In ",(0,i.jsx)(e.strong,{children:"Week 11: LLM Planning"}),", you'll use GPT-4 to generate multi-step task plans, implementing the ReAct pattern for adaptive reasoning during execution."]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);