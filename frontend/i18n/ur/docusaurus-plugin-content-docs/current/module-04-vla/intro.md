# ماڈیول 4: وژن-لینگویج-ایکشن انضمام

## جائزہ

ماڈیول 4 میں خوش آمدید، جہاں ہم Large Language Models (LLMs) کے روبوٹکس کے ساتھ اتحاد کو تلاش کرتے ہیں تاکہ جسد کے AI ایجنٹس تیار کیے جا سکیں جو انسانی نیت کو سمجھنے، اپنے ماحول کا ادراک کرنے، اور جسمانی اعمال انجام دینے کے قابل ہوں۔ یہ ماڈیول ہیومنوائڈ روبوٹکس کی حد کی نمائندگی کرتا ہے— سسٹم جو صرف پیش پروگرام کردہ رoutines کو فالو نہیں کرتے، بلکہ قدرتی زبان کے حکم کو سمجھتے ہیں، وژوئل مناظر کے بارے میں استدلال کرتے ہیں، اور موشن سیکوئنس کو مطابقت پذیر بناتے ہیں۔

وژن-لینگویج-ایکشن (VLA) ماڈلز تین اہم اقسام کو متحد کرتے ہیں: وژوئل ادراک (کیمرز، ڈیپتھ سینسرز)، قدرتی زبان کی سمجھ (تقریر، متن)، اور موٹر کنٹرول (جوائنٹ ایکٹو ایشن، گریسنگ)۔ روایتی روبوٹکس پائپ لائنز کے برعکس جہاں ادراک، منصوبہ بندی، اور کنٹرول آزادانہ طور پر کام کرتے ہیں، VLA آرکیٹیکچر مشترکہ نمائندگیوں کو سیکھتے ہیں جو براہ راست متعدد ماڈلز کے ان پٹس سے روبوٹ ایکشنز میں نکال دیتے ہیں۔ یہ اینڈ-ٹو-اینڈ نقطہ نظر "بائیں الماری پر سرخ مگ کو اٹھاؤ" جیسے رویوں کو فعال کرتا ہے— جس کے لئے وژوئل گراؤنڈنگ (شناخت "سرخ مگ")، جگہ کی استدلال ("بائیں الماری")، اور مینوپولیشن منصوبہ بندی (گریس ٹریجیکٹری) کی ضرورت ہوتی ہے۔

## VLA کیا ہے؟

![VLA آرکیٹیکچر](/img/modules/module-04/vla-architecture.svg)
*شکل 4.1: وژن-لینگویج-ایکشن آرکیٹیکچر متعدد ماڈلز کے ان پٹس (آواز، وژوئل، اشارہ، متن) کو دکھاتا ہے جو GPT-4 کے ذریعے کام کی منصوبہ بندی اور ROS 2 کے ذریعے انجام دہی کے لئے بہتا ہے*

**وژن-لینگویج-ایکشن (VLA)** ایسے نیورل آرکیٹیکچر کو بیان کرتا ہے جو وژوئل مشاہدات اور زبان کی ہدایات کو پروسیس کرتا ہے تاکہ روبوٹ کنٹرول کمانڈز پیدا کیے جا سکیں۔ بنیادی ابتكار ایک مشترکہ ایمبیڈنگ سپیس ہے جہاں:

- **وژن ایمبیڈنگز** 3D منظر جیومیٹری، آبجیکٹ سیمینٹکس، اور جگہ کے تعلقات کو قبضہ کرتے ہیں
- **زبان کی ایمبیڈنگز** کام کے اہداف، پابندیوں، اور وقت کی ترتیب کو کوڈ کرتے ہیں ("پہلے دراز کھولو، پھر گرفت کرو")
- **ایکشن ایمبیڈنگز** مشترکہ یا کام کی جگہ میں قابل عمل روبوٹ ٹریجیکٹریز کی نمائندگی کرتے ہیں

RT-2 (روبوٹکس ٹرانسفارمر 2) اور PaLM-E جیسے جدید VLA ماڈلز پیش تربیت شدہ وژن-زبان ماڈلز (CLIP، GPT-4V) کو استعمال کرتے ہیں اور روبوٹ ڈیموسٹریشن ڈیٹا پر ان کو فائن ٹیون کرتے ہیں۔ یہ ٹرانسفر لرننگ نقطہ نظر روبوٹس کو انٹرنیٹ سکیل علم سے فائدہ اٹھانے کی اجازت دیتا ہے— ایک روبوٹ جس نے تربیت میں کبھی "وہسک" نہیں دیکھا وہ اب بھی ایک کو پہچان سکتا ہے کیونکہ کچن اوزاروں کے بارے میں زبان کے ماڈل کی سمجھ کا استعمال کرتے ہوئے۔

## جسد کے AI ایجنٹس

**جسد کے AI** اس بات کو زور دیتا ہے کہ ذہانت جسمانی دنیا کے ساتھ تعامل سے پیدا ہوتی ہے، صرف علامتی استدلال سے نہیں۔ ایک جسد کا ایجنٹ کو ضرور:

1. **زبان کو ادراک میں زمین دینا**: "نیلا ڈبہ" کو ڈھونڈنا ہوتا ہے جس میں رنگ اور شکل کی وضاحت کے ساتھ اشیاء کی شناخت ہو
2. **دنیا کے ماڈلز کو برقرار رکھنا**: وقت کے ساتھ اشیاء کی حالت کو ٹریک کرنا (اب دراز کھلا ہے، مگ کو گرفت کر لیا گیا ہے)
3. **بندش والے کنٹرول کو انجام دینا**: حسی فیڈ بیک کے مطابق اعمال کو مطابقت دینا (اگر چیز پھسل رہی ہو تو گرفت کے زور کو مطابقت دیں)
4. **اٹھانے کی عدم یقینی**: جزوی مشاہدات، سینسر نوائس، اور متحرک ماحول سے بازیافت کرنا

روایتی AI سسٹم انتہائی علامات پر کام کرتے ہیں؛ جسد کے AI ایجنٹس پکسلز، پوائنٹ کلاؤڈز، اور ٹورکو کمانڈز پر کام کرتے ہیں۔ یہ زمین داری ماڈلز کو فزکس-آگاہ نمائندگیاں سیکھنے پر مجبور کرتی ہے— سمجھنا کہ ایک بھاری چیز کو دھکیلنا ہلکی چیز سے زیادہ زور کا تقاضا کرتا ہے، یا مگ کو ہینڈل سے پکڑنا کنارے کی بجائے زیادہ مستحکم ہے۔

## متعدد ماڈلز کی سیکھنے کی صلاحیت

VLA سسٹم متعدد ماڈلز کی ڈیٹا سٹریمز کو مربوط کرنے کے لئے **متعدد ماڈلز کی سیکھنے کی صلاحیت** کا استعمال کرتے ہیں۔ کلیدی تکنیکیں شامل ہیں:

**کراس-ماڈل اٹینشن**: ٹرانسفارمر آرکیٹیکچر جو وژن اور زبان کے ٹوکن پر مشترکہ طور پر توجہ دیتے ہیں۔ "بائیں سیب اٹھاؤ" کو پروسیس کرتے وقت، توجہ کے وزن بائیں سیب کے مطابق وژوئل ٹوکن پر توجہ مرکوز کریں گے، نہ کہ دائیں طرف کے پھلوں پر۔

**کنٹراسٹو لرننگ**: CLIP جیسی تکنیکیں وژن-زبان کے مطابقت کو اس طرح سے سیکھتی ہیں کہ مثبت جوڑوں (تصویر + درست کیپشن) کو منفی جوڑوں (تصویر + غلط کیپشنز) کے خلاف مقابلہ کیا جاتا ہے۔ یہ ایک مشترکہ ایمبیڈنگ سپیس بناتا ہے جہاں معنی کے اعتبار سے مماثل تصورات ایک دوسرے کے قریب ہوتے ہیں۔

**ایکشن ٹوکنائزیشن**: روبوٹ ایکشنز کو الگ الگ ٹوکن کے طور پر ظاہر کرنا زبان کے ماڈلز کو "روبوٹ بولنے" کے قابل بناتا ہے— ایکشن سیکوئنس کو خودکار طور پر متن کی تخلیق کی طرح پیدا کرنا۔ RT-2 جاری جوائنٹ پوزیشنز کو 256 بنس میں الگ کرتا ہے، ہر کنفیگریشن کو ایک لغت ٹوکن کے طور پر سمجھتا ہے۔

**ہیئرآرکیکل پالیسیز**: LLMs بلند سطح کے کام کی منصوبہ بندی پیدا کرتے ہیں ("کچن میں جاؤ، فریج کھولو، دودھ کو گرفت کرو") جبکہ کم سطح کے کنٹرولرز موشن بنیادیات کو سنبھالتے ہیں۔ یہ ذمہ داریوں کا الگ ہونا نمونہ کی کارکردگی میں بہتری لاتا ہے— LLM کو کم سطح کے PID کنٹرول سیکھنے کی ضرورت نہیں ہوتی۔

## ماڈیول کی ساخت

یہ ماڈیول 3 ہفتوں میں ترقی کرتا ہے:

**ہفتہ 11: آواز اور زبان کا انضمام**
- OpenAI Whisper تقریر سے متن میں تبدیلی کے لئے
- قدرتی زبان کے حکم سے نیت کی درجہ بندی
- GPT-4 کام کی منصوبہ بندی اور سوچنے کی زنجیر کے لئے
- روبوٹ کنٹرول کے لئے پرومپٹ انجینئرنگ

**ہفتہ 12: متعدد ماڈلز کا ادراک**
- CLIP کے لئے صفر-شان وژوئل گراؤنڈنگ
- حوالہ دینے والے اظہار کی سمجھ ("سرخ ڈبہ کے بائیں طرف کی چیز")
- MediaPipe گیسچر کی شناخت اور انسان-روبوٹ تعامل کے لئے
- منظر کی سمجھ کے لئے وژوئل سوال جواب

**ہفتہ 13: کیپسٹون انضمام**
- اینڈ-ٹو-اینڈ سسٹم: آواز کا حکم → LLM منصوبہ بندی → نیوی گیشن → مینوپولیشن → فیڈ بیک
- گفتگو کرنے والے ہیومنوائڈ کے ساتھ خودکار
- ریل ٹائم متعدد ماڈلز کا اتحاد
- جسمانی ہیومنوائڈ پلیٹ فارم پر نافذ کرنا

## شرائط

اس ماڈیول میں کامیاب ہونے کے لئے، آپ کو ہونا چاہیے:

- **گہری سیکھنے کی بنیاد**: نیورل نیٹ ورکس، ٹرانسفارمرز، توجہ کے میکنزمز کی سمجھ
- **Python مہارت**: PyTorch/TensorFlow، NumPy، API انضمام کا تجربہ
- **ROS 2 علم**: ماڈیول 1-3 (نیوی گیشن، مینوپولیشن کی بنیادیں) کا اختتام
- **وژن کی بنیاد**: تصویر کی پروسیسنگ، آبجیکٹ ڈیٹیکشن، کیمرہ کیلبریشن کے ساتھ واقفیت
- **API رسائی**: GPT-4 اور Whisper کے لئے OpenAI API کلید (سیٹ اپ ہدایات دیکھیں)

## VLA ہیومنوائڈ روبوٹکس کے لئے کیوں؟

ہیومنوائڈ روبوٹس کو انسانوں کے مراکز والے ماحول میں کام کرنا ہوتا ہے جو قدرتی زبان کے تعامل کے لئے ڈیزائن کیے گئے ہیں۔ روزمرہ کے کاموں پر غور کریں:

- **"مجھے دفتر کی میز سے میرا لیپ ٹاپ لے دو"**: مقام کے حوالہ جات کو تحلیل کرنا، کمرے کے ذریعے وژوئل تلاش، نیوی گیشن منصوبہ بندی، اور محفوظ ہینڈ اوور کی ضرورت ہوتی ہے
- **"میری رات کا کھانا تیار کرنے میں مدد کرو"**: کھلا تعاون جسے سیاقی سمجھ کی ضرورت ہوتی ہے (رات کا کھانا کچن کا مطلب ہے، کھانے کی تیاری کے اوزار)
- **"پیکج بہت بھاری ہے، کیا تم مدد کر سکتے ہو؟"**: انسان کی درخواست سے اچانک کام کی شروعات، تعاونی اٹھانے کے لئے زور کا ادراک

پیش پروگرام کردہ محدود حالت کی مشینیں اس متغیر کو سنبھال نہیں سکتیں۔ VLA ماڈلز عام کارکردگی کی پالیسیاں سیکھتے ہیں جو ادراک، زبان، اور ایکشن کو جوڑتے ہیں— واقعی خودکار ہیومنوائڈ معاونین کی بنیاد۔

## اخلاقی مسائل

جیسے جیسے آپ VLA سسٹم تیار کرتے ہیں، ان باتوں کا احساس رکھیں:

- **رازداری**: تقریر اور کیمرہ ڈیٹا حساس معلومات پر مشتمل ہوتا ہے؛ محفوظ ڈیٹا ہینڈلنگ نافذ کریں
- **حفاظت**: LLM ہیلیوسینیشن نقصان دہ روبوٹ ایکشنز پیدا کر سکتے ہیں؛ ہمیشہ منصوبہ بند ٹریجیکٹریز کی توثیق کریں
- **رُجحان**: زبان کے ماڈلز ڈیٹا سیٹ کے رجحانات کو ورثے میں پاتے ہیں؛ مختلف جماعتیں اور زبانوں میں جانچ کریں
- **شفافیت**: اہم کاموں کے لئے انسانی نگرانی برقرار رکھیں؛ ہنگامی بند کرنے کے میکنزم نافذ کریں

## شروع کرنا

یقینی بنائیں کہ آپ کے پاس ہے:
- Ubuntu 22.04 ROS 2 Humble کے ساتھ (ماڈیول 1 سے)
- Python 3.10+ PyTorch 2.0+ کے ساتھ
- OpenAI API کلید (سیٹ کریں `OPENAI_API_KEY` ماحولیاتی متغیر کے طور پر)
- 8GB+ VRAM کے ساتھ GPU (NVIDIA RTX 3060 یا بہتر تجویز کردہ)
- جانچ کے لئے ویب کیمرہ اور مائیکروفون

گفتگو کرنے والے روبوٹس بنانے کے لئے تیار؟ **ہفتہ 11: وائس ٹو ایکشن** پر آگے بڑھیں تاکہ تقریر کی شناخت اور کمانڈ کی تحلیل نافذ کریں۔